{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bcf52014",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Entity embeddings of categorical variables\"\n",
    "date: \"2023-09-19\"\n",
    "author: Luca Papariello\n",
    "categories:\n",
    "- regression\n",
    "- random forests\n",
    "- embeddings\n",
    "- pytorch\n",
    "description: Bringing the power of neural nets to tree-based models.\n",
    "toc: true\n",
    "image: nn_rf.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74aafb5",
   "metadata": {},
   "source": [
    "[![](https://kaggle.com/static/images/open-in-kaggle.svg){fig-align=\"right\"}](https://www.kaggle.com/code/lucapapariello/entity-embeddings-of-categorical-variables/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c45afd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Despite the buzz around generative AI, most applications in industry still originate from tabular datasets. In tabular data, some columns may represent numerical variables, like *atmospheric pressure*, while others may be categorical variables, like *sex* or *product categories*. These can take only a limited number of values. The path to using numerical variables is relatively smooth and requires little preprocessing (for some algorithms, even none at all). In contrast, categorical variables must first be converted into numbers, as this is what a computer can process, and this can be done in many ways.\n",
    "\n",
    "Two conventional approaches to encode categorical variables are **ordinal encoding** and **one-hot encoding**. Ordinal encoding assigns each unique value to a different integer and, as such, assumes an ordering of the categories. For instance, \"Size\" could be the name of a categorical columns with values: *small* < *medium* < *large*, which would be mapped to 0, 1, and 2, respectively. One-hot encoding creates instead a new column for each possible value in the original data indicating its presence or absence. If the first sample in the dataset were *large*, we would get the following one-hot representation: (0, 0, 1). Unlike ordinal encoding, one-hot encoding does not assume an ordering of the categories.\n",
    "\n",
    "However, both ordinal and one-hot encodings have their limitations. To name a few, in ordinal encoding, the assigned numerical values may introduce unintended relationships or orders between categories that do *not* exist in the original data. One-hot encoding, while solving the issue of introducing unintended order, can lead to high-dimensional and sparse representations for features with a large number of unique categories. This translates into increased computational complexity and memory usage.\n",
    "\n",
    "In 2015, the [Rossmann sales competition](https://www.kaggle.com/c/rossmann-store-sales) took place on Kaggle. The solution of one of the gold medal winners clearly diverged from the others by using a deep learning model, in one of the first known examples of a cutting-edge deep learning model for tabular data. Rather than using traditional encoding methods, the authors introduced the concept of **Entity Embeddings**. Entity Embeddings provide a way to represent categorical variables as *low-dimensional continuous* vectors, capturing the underlying semantic relationships between categories. This approach eliminates the limitations of ordinal encoding's introduced order and one-hot encoding's curse of dimensionality. Their approach is summarised in the paper [Entity Embeddings of Categorical Variables](https://arxiv.org/abs/1604.06737), by Cheng Guo and Felix Berkhahn.\n",
    "\n",
    "We'll explore here the concepts and advantages of Entity Embeddings, diving deep and trying to replicate the main findings of the paper. Along the way, we'll offer a comprehensive understanding of their applications in general machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496e61b",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We use the very same dataset of the Rossmann sales competition that was used by the authors of the paper&mdash;this is readily available on Kaggle. The dataset consists of two parts. The first one is `train.csv` and comprises of daily sales data for several different stores, while the second one is `store.csv` and provides additional details about each of these  stores. Since the focus here is not on obtaining the best possible result, but on the representation of features and the introduction of Entity Embeddings, we'll restrict our attention to the base features provided in the first file. Here is a snippet of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310de1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rossmann-store-sales.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "\n",
    "if iskaggle: \n",
    "    path = Path('../input/rossmann-store-sales')\n",
    "else:\n",
    "    import kaggle\n",
    "    path = Path('rossmann-store-sales')\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    shutil.unpack_archive(str(path)+'.zip', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c2a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>5263</td>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>6064</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>8314</td>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek       Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
       "0      1          5 2015-07-31   5263        555     1      1            0   \n",
       "1      2          5 2015-07-31   6064        625     1      1            0   \n",
       "2      3          5 2015-07-31   8314        821     1      1            0   \n",
       "\n",
       "   SchoolHoliday  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = path/'train.csv'\n",
    "train_df = pd.read_csv(train_data, parse_dates=['Date'], low_memory=False)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43803943",
   "metadata": {},
   "source": [
    "Most of the fields are self-explanatory and their description can be found on the competition webpage. What we need to know here is that `Sales` is our *target variable* and represents the turnover on a given day. Apart from the column `Date`, which is a type in its own right, and `Customers`, which is not available at test time and will hence not be considered, all the features of this dataset are categorical. It should be noted that while most of them have a low cardinality, `Store`, which is a unique ID identifying each shop, can take a whopping 1115 different values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1eee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Store  DayOfWeek       Date  Open  Promo StateHoliday  SchoolHoliday\n",
       "0   1      1          4 2015-09-17   1.0      1            0              0\n",
       "1   2      3          4 2015-09-17   1.0      1            0              0\n",
       "2   3      7          4 2015-09-17   1.0      1            0              0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "test_data = path/'test.csv'\n",
    "test_df = pd.read_csv(test_data, parse_dates=['Date'], low_memory=False)\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e622f5",
   "metadata": {},
   "source": [
    "Before moving on, we get rid of closed stores, which have zero sales, as we'll not make any predictions on them at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0228c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "to_keep = ~((train_df.Open == 0) & (train_df.Sales == 0))\n",
    "train_df = train_df.loc[to_keep, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c3764",
   "metadata": {},
   "source": [
    "## Train/Valid split\n",
    "\n",
    "Our training data spans approximately 2.5 years from 2013 to mid-2015. The test data covers instead the subsequent part of 2015, with no overlap with the training set dates. To test the generalisation capability of our model, we'll try to put ourselves in the closest possible situation to this by sorting the training data by `Date` and keeping the last 10% of the samples for validation. In this way, we use the older samples for training and the most recent ones for validation. This should ensure that the performance observed on the validation set is as close as possible to that of the leaderboard after submission.\n",
    "\n",
    "We'll also drop samples from the validation set for which there are zero sales. This is to align ourselves with the competition, whose website states that \"Any day and store with 0 sales is ignored in scoring.\".^[Note that our main evaluation metric, the RMSPE, would diverge when the target is zero.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc872fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def drop_zero_sales(df: pd.DataFrame, ps: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"Return samples where targets (`Sales`) are not zero.\"\n",
    "    to_drop = (ps == 0)  # Sales = 0\n",
    "    return df[~to_drop], ps[~to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e0dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 759952\n",
      "Valid size:  84439\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "tgt = 'Sales'  # Target variable\n",
    "train_df = train_df.sort_values(by=['Date']).reset_index(drop=True)\n",
    "\n",
    "train_xs, valid_xs, train_y, valid_y = train_test_split(\n",
    "    train_df.drop(columns=tgt), train_df[tgt], test_size=0.1, shuffle=False\n",
    ")\n",
    "\n",
    "valid_xs, valid_y = drop_zero_sales(valid_xs, valid_y)\n",
    "\n",
    "print(f'Train size: {len(train_y):>6}')\n",
    "print(f'Valid size: {len(valid_y):>6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043612f",
   "metadata": {},
   "source": [
    "## Ordinal encoding\n",
    "\n",
    "As a first (and only) operation to prepare the data, we enrich the representation of dates. We keep it simple and only create new columns for `Day`, `Month`, and `Year` out of `Date`. After all, there are already the `DayOfWeek`, `StateHoliday`, and `SchoolHoliday` columns providing additional information in this regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def proc_data(df: pd.DataFrame):\n",
    "    \"Process DataFrame and create date features inplace.\"\n",
    "    df['Day'] = df.Date.dt.day\n",
    "    df['Month'] = df.Date.dt.month\n",
    "    df['Year'] = df.Date.dt.year\n",
    "    df['Open'] = df.Open.fillna(1).astype(int)\n",
    "\n",
    "proc_data(train_xs)\n",
    "proc_data(valid_xs)\n",
    "proc_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172da1a6",
   "metadata": {},
   "source": [
    "We're now ready to map our categorical variables to integers using scikit-learn's `OrdinalEncoder`. In addition, we only select the relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd88d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['Store', 'DayOfWeek', 'Day', 'Month', 'Year', 'Promo', 'StateHoliday', 'SchoolHoliday']\n",
    "\n",
    "oe = OrdinalEncoder(dtype=int)\n",
    "train_xs[cats] = oe.fit_transform(train_xs[cats])\n",
    "valid_xs[cats] = oe.transform(valid_xs[cats])\n",
    "test_df[cats] = oe.transform(test_df[cats])\n",
    "\n",
    "train_xs, valid_xs, test_xs = train_xs[cats], valid_xs[cats], test_df[cats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32db968",
   "metadata": {},
   "source": [
    "Let's have a look at our preprocessed dataframe, which is now ready to be passed to our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f8d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>759949</th>\n",
       "      <td>351</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759950</th>\n",
       "      <td>350</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759951</th>\n",
       "      <td>364</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  DayOfWeek  Day  Month  Year  Promo  StateHoliday  SchoolHoliday\n",
       "759949    351          5    1      4     2      0             0              0\n",
       "759950    350          5    1      4     2      0             0              0\n",
       "759951    364          5    1      4     2      0             0              0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xs.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129bbb37",
   "metadata": {},
   "source": [
    "# Evaluation metric\n",
    "\n",
    "Before moving on to the model, we want to take a look at one last important piece: the *evaluation metric*.\n",
    "Submissions for this Kaggle competitions are evaluated in terms of *Root Mean Square Percentage Error* (RMSPE), so we'll use the same metric here. Additionally, we pick another common evaluation metric, the *Mean Absolute Percentage Error* (MAPE). This is the metric used in the publication and will allow us to benchmark our results against theirs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de19357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def rmspe(pred: np.array, y: np.array) -> float:\n",
    "    \"Compute Root Mean Square Percentage Error given predictions and targets.\"\n",
    "    return math.sqrt((((pred-y)/y)**2).mean())\n",
    "\n",
    "def m_rmspe(m, xs: np.array, y: np.array) -> float:\n",
    "    \"Compute Root Mean Square Percentage Error given model, samples, and targets.\"\n",
    "    return rmspe(m.predict(xs), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def mape(pred: np.array, y: np.array) -> float:\n",
    "    \"Compute Mean Absolute Percentage Error given predictions and targets.\"\n",
    "    relative_err = np.abs((y - pred) / y)\n",
    "    result = np.sum(relative_err) / len(y)\n",
    "    return result\n",
    "\n",
    "def m_mape(m, xs: np.array, y: np.array) -> float:\n",
    "    \"Compute Mean Absolute Percentage Error given model, samples, and targets.\"\n",
    "    return mape(m.predict(xs), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1170868",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "We choose Random Forests as our starting point, which is an ensemble of decision trees. Reason for this is that it requires little preprocessing, it isn't very sensitive to hyperparameters, and generally provides a strong baseline.\n",
    "\n",
    "For convenience, we define an `rf` function that returns a *fitted* Random Forests regression model. We are now ready to fit our first model and check its performance, which is expressed in terms of RMSPE (the competition metric) and MAPE (the metric used in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def rf(xs, y, n_estimators=30, max_samples=200_000, max_depth=35, min_samples_leaf=5, **kwargs):\n",
    "    \"Return a fitted Random Forests regression model.\"\n",
    "    return RandomForestRegressor(\n",
    "        n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples,\n",
    "        max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=34, **kwargs\n",
    "    ).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120709e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = rf(train_xs.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed252808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid RMSPE: 0.310\n",
      "Valid MAPE:  0.185\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "print(f\"Valid RMSPE: {m_rmspe(m, valid_xs.values, valid_y.values):.3f}\")\n",
    "print(f\"Valid MAPE:  {m_mape(m, valid_xs.values, valid_y.values):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ba369",
   "metadata": {},
   "source": [
    "Submitting the results of this model would result in a score on the *private test* set of about 0.303&mdash;note how close this is to the score on our validation set! This is good news because it suggests that we can trust our validation set. However, this result would put us a long way from the top of the leaderboard, where the winner had an impressive score of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6c3dc",
   "metadata": {},
   "source": [
    "# Entity embeddings\n",
    "\n",
    "Embeddings are at the heart of many recommender systems, particularly in cases where the possibility of employing a content-based approach is ruled out due to the lack of information about users and items. Collaborative filtering offers a (somewhat surprising) solution for predicting user preferences based only on the interests of other users.\n",
    "The key idea here is that of **latent dimensions**, which are features that describe users and items and are *automatically* discovered by a model. The resulting matrices containing the latent factors of users and items are exactly the user and item embedding matrices. Typically, they are determined via *low-rank matrix factorisation* or randomly initialised and improved during the *training of a neural network*.\n",
    "\n",
    "The entity embeddings we discuss today generalise this same idea to any categorical variable, i.e. we want to learn a low-dimensional vector representation for each category in a given categorical feature. We'll optimise these embeddings during the training process along with the rest of the model's parameters. The underlying intuition is that by representing categories as continuous vectors, the model can learn meaningful relationships and similarities between categories through their proximity in the embedding space. This allows the model to capture complex interactions and dependencies among categorical variables, which can be very valuable for predictive tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388701c",
   "metadata": {},
   "source": [
    "## Training the embeddings\n",
    "\n",
    "The deep learning framework we will use to build and train our neural network is PyTorch. In PyTorch, a Dataset is constructed by subclassing `Dataset` and requires us to override two dunder methods: `__getitem__` and `__len__`. In the following we implement our Dataset to return a tensor for the features and one for the target for the training and validation set, whereas only the features for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "class TabDataset(Dataset):\n",
    "    \"Tabular Dataset that yields categorical features and target.\"\n",
    "\n",
    "    def __init__(self, feats, new_data=True, tgt=None):\n",
    "        self.x_cat = torch.tensor(feats.values, dtype=torch.int32)\n",
    "        self.n_samples = len(feats)\n",
    "        self.new_data = new_data\n",
    "        \n",
    "        if not new_data:\n",
    "            self.y = torch.tensor(tgt.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if not self.new_data:\n",
    "            return self.x_cat[idx], self.y[idx]\n",
    "        return self.x_cat[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d49f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TabDataset(train_xs, new_data=False, tgt=train_y)\n",
    "valid_ds = TabDataset(valid_xs, new_data=False, tgt=valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daac8ed",
   "metadata": {},
   "source": [
    "Next, we can build the training and validation `DataLoader`s, which take a Dataset and return an iterable that handles shuffling, batching, and all the rest for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1024  # Batch size\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40208bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "class LinReLu(nn.Sequential):\n",
    "    \"Module grouping `Linear` layer and `ReLu` activation.\"\n",
    "\n",
    "    def __init__(self, n_in, n_out, act=None):\n",
    "        layers = [nn.Linear(n_in, n_out)]\n",
    "        if act is not None: layers.append(act)\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ea76b",
   "metadata": {},
   "source": [
    "Now comes the main part, the construction of the neural network that will learn the entity embeddings. In PyTorch, we can easily create custom models by subclassing the `nn.Module` module. In a nutshell, this neural network consists of an initial part of embedding layers&mdash;these represent our matrices of latent factors&mdash;followed by a fully connected part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb193d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularNeuralNet(nn.Module):\n",
    "    \"Neural network model for tabular data.\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_szs: list,  # List of (unique_cats, embedding_dim)\n",
    "        out_sz: int,    # Number of outputs for final layer\n",
    "        layers: list,   # Size of the hidden layers\n",
    "        act_cls=nn.ReLU(inplace=True),  # Activation type after `Linear` layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_szs])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_emb = n_emb\n",
    "        sizes = [n_emb] + layers + [out_sz]\n",
    "        actns = [act_cls for _ in range(len(sizes) - 2)] + [None]\n",
    "        _layers = [LinReLu(sizes[i], sizes[i + 1], act=a) for i, a in enumerate(actns)]\n",
    "        self.layers = nn.Sequential(*_layers)\n",
    "\n",
    "    def forward(self, x_cat):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeds)]\n",
    "        x = torch.cat(x, 1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93236746",
   "metadata": {},
   "source": [
    "The only thing that remains for us to do is to determine the `emb_szs` parameter. The part concerning the number of unique categories can easily be found using the `OrdinalEncoder` we fitted above, but the embedding size for each categorical feature is a free parameter. As such, there is no clear-cut way to determine it but it's typically *much smaller* than the number of unique categories. We are thus thankful that the authors report these values in their paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa640ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "num_unique = [len(c) for c in oe.categories_]\n",
    "emb_dims = [10, 6, 10, 6, 2, 1, 3, 1]\n",
    "embed_sizes = [(u, e) for u, e in zip(num_unique, emb_dims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = TabularNeuralNet(emb_szs=embed_sizes, out_sz=1, layers=[200, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef117f4d",
   "metadata": {},
   "source": [
    "What follows is the classic PyTorch training loop, which we encapsulate for convenience in two functions, `train_one_epoch` and `validate_one_epoch`, that do what they say: train and validate the model over one epoch, respectively. We train our model for a few epochs using the standard Mean Squared Error (MSE) loss function and Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def train_one_epoch(train_loader, model, loss_fn, optim, progress_bar=None):\n",
    "    \"Train model over one epoch.\"\n",
    "\n",
    "    model.train()  # Put model in training mode\n",
    "    dset_size = len(train_loader.dataset)  # Train set size\n",
    "\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        # Forward pass and loss\n",
    "        outputs = model(data)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if progress_bar is not None: progress_bar.update(1)\n",
    "\n",
    "        # Print progress every X batches\n",
    "        if i % 400 == 0:\n",
    "            loss, step = loss.item(), i * len(labels)\n",
    "            print(f\"Loss: {loss:>8.0f}  [{step:>6d}/{dset_size:>6d}]\")\n",
    "        elif i == len(train_loader) - 1:\n",
    "            loss = loss.item()\n",
    "            print(f\"Loss: {int(loss):>8}  [{dset_size:>6d}/{dset_size:>6d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def validate_one_epoch(val_loader, model, loss_func):\n",
    "    \"Validate model over one epoch.\"\n",
    "    \n",
    "    model.eval()  # Put model in eval mode\n",
    "    num_batches = len(val_loader)\n",
    "    \n",
    "    valid_loss, valid_rmspe = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader:\n",
    "            outputs = model(data)\n",
    "            valid_loss += loss_func(outputs, targets).item()\n",
    "            valid_rmspe += rmspe(outputs, targets)\n",
    "\n",
    "    valid_loss /= num_batches   # Avg. loss\n",
    "    valid_rmspe /= num_batches  # Avg. RMSPE\n",
    "    print(f\"Avg. valid. loss: {int(valid_loss)}, Avg. RMSPE: {valid_rmspe:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08064455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Hyperparameters\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ef8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Loss function and optimiser\n",
    "loss_func = nn.MSELoss()\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd1588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Loss: 56439536  [     0/759952]\n",
      "Loss:  1829678  [409600/759952]\n",
      "Loss:  2018285  [759952/759952]\n",
      "Avg. valid. loss: 1823780, Avg. RMSPE: 0.2195\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Loss:  1885142  [     0/759952]\n",
      "Loss:  1538159  [409600/759952]\n",
      "Loss:  1631149  [759952/759952]\n",
      "Avg. valid. loss: 1681688, Avg. RMSPE: 0.2059\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Loss:  1414050  [     0/759952]\n",
      "Loss:  1237316  [409600/759952]\n",
      "Loss:   899826  [759952/759952]\n",
      "Avg. valid. loss: 1374336, Avg. RMSPE: 0.1720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(EPOCHS):\n",
    "    print(f\"Epoch {ep+1}\\n\" + \"-\"*31)\n",
    "    train_one_epoch(train_dl, nn_model, loss_func, optim)\n",
    "    validate_one_epoch(valid_dl, nn_model, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bccb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in valid_dl:\n",
    "        outputs = nn_model(data)\n",
    "        all_preds.append(outputs)\n",
    "\n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85422e3c",
   "metadata": {},
   "source": [
    "That's it! By training our neural network we obtained the entity embeddings. As a nice side-product we also got a fully functioning model that we can use to forecast the sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f7082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid RMSPE: 0.179\n",
      "Valid MAPE:  0.123\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "print(f\"Valid RMSPE: {rmspe(all_preds, valid_y.values.reshape(-1, 1)):.3f}\")\n",
    "print(f\"Valid MAPE:  {mape(all_preds, valid_y.values.reshape(-1, 1)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1d3de",
   "metadata": {},
   "source": [
    "It turns out that this model works much better than our previous baseline, lowering both RMSPE and MAPE substantially. \n",
    "Submitting these results would bring the score on the *private test* set down to about 0.17. We are definitely moving in the right direction!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06bc19",
   "metadata": {},
   "source": [
    "## Random Forests with Entity Embeddings\n",
    "\n",
    "Now that we have trained the neural network, we have the entity embeddings at our disposal. All that remains is to extract them from the embedding layer of our `TabularNeuralNet`. We write a function, `embed_features`, to do that since we'll have to repeat this operation for the training, validation and test set^[If you come from the fastai world, check out [this blog](https://towardsdatascience.com/entity-embeddings-for-ml-2387eb68e49) on Medium where this process is done using fastai objects (TabularPandas, Learner, etc.).]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def embed_features(model, xs, encoder):\n",
    "    \"Replace categorical columns in `xs` w/ embeddings extracted from `model`.\"\n",
    "    xs = xs.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, col in enumerate(encoder.feature_names_in_):\n",
    "            # Get embedding matrix\n",
    "            emb = model.embeds[i]\n",
    "            emb_data = emb(torch.tensor(xs[col].values, dtype=torch.int32))\n",
    "            emb_names = [f'{col}_{j}' for j in range(emb_data.shape[1])]\n",
    "\n",
    "            # Replace old feature col. w/ new one(s)\n",
    "            feat_df = pd.DataFrame(\n",
    "                data=emb_data.cpu().numpy(), index=xs.index, columns=emb_names\n",
    "            )\n",
    "            xs = xs.drop(col, axis=1).join(feat_df)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c45af8",
   "metadata": {},
   "source": [
    "We are now ready to apply the mapping defined by the embeddings to our original features. We also show the first few rows to see what the result looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train embeddings\n",
    "emb_train_xs = embed_features(nn_model, train_xs, oe)\n",
    "\n",
    "# Validation embeddings\n",
    "emb_valid_xs = embed_features(nn_model, valid_xs, oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aff022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store_0</th>\n",
       "      <th>Store_1</th>\n",
       "      <th>Store_2</th>\n",
       "      <th>Store_3</th>\n",
       "      <th>Store_4</th>\n",
       "      <th>Store_5</th>\n",
       "      <th>Store_6</th>\n",
       "      <th>Store_7</th>\n",
       "      <th>Store_8</th>\n",
       "      <th>Store_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Month_3</th>\n",
       "      <th>Month_4</th>\n",
       "      <th>Month_5</th>\n",
       "      <th>Year_0</th>\n",
       "      <th>Year_1</th>\n",
       "      <th>Promo_0</th>\n",
       "      <th>StateHoliday_0</th>\n",
       "      <th>StateHoliday_1</th>\n",
       "      <th>StateHoliday_2</th>\n",
       "      <th>SchoolHoliday_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.464628</td>\n",
       "      <td>1.389546</td>\n",
       "      <td>0.031453</td>\n",
       "      <td>0.837222</td>\n",
       "      <td>-2.890536</td>\n",
       "      <td>-1.271165</td>\n",
       "      <td>0.351642</td>\n",
       "      <td>0.352165</td>\n",
       "      <td>-1.067315</td>\n",
       "      <td>0.432503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912523</td>\n",
       "      <td>1.552763</td>\n",
       "      <td>-0.231177</td>\n",
       "      <td>-0.952061</td>\n",
       "      <td>0.123717</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>0.693458</td>\n",
       "      <td>-1.862958</td>\n",
       "      <td>-0.175023</td>\n",
       "      <td>-1.131267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.548922</td>\n",
       "      <td>0.104170</td>\n",
       "      <td>0.247237</td>\n",
       "      <td>0.384087</td>\n",
       "      <td>-1.878805</td>\n",
       "      <td>-1.581414</td>\n",
       "      <td>1.528581</td>\n",
       "      <td>-0.838187</td>\n",
       "      <td>-1.658705</td>\n",
       "      <td>-2.186104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912523</td>\n",
       "      <td>1.552763</td>\n",
       "      <td>-0.231177</td>\n",
       "      <td>-0.952061</td>\n",
       "      <td>0.123717</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>0.693458</td>\n",
       "      <td>-1.862958</td>\n",
       "      <td>-0.175023</td>\n",
       "      <td>-1.131267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.613926</td>\n",
       "      <td>-0.435071</td>\n",
       "      <td>-0.327684</td>\n",
       "      <td>2.248206</td>\n",
       "      <td>-0.140331</td>\n",
       "      <td>-0.281201</td>\n",
       "      <td>0.297878</td>\n",
       "      <td>2.848119</td>\n",
       "      <td>1.672459</td>\n",
       "      <td>1.167650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912523</td>\n",
       "      <td>1.552763</td>\n",
       "      <td>-0.231177</td>\n",
       "      <td>-0.952061</td>\n",
       "      <td>0.123717</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>0.693458</td>\n",
       "      <td>-1.862958</td>\n",
       "      <td>-0.175023</td>\n",
       "      <td>-1.131267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Store_0   Store_1   Store_2   Store_3   Store_4   Store_5   Store_6  \\\n",
       "0 -0.464628  1.389546  0.031453  0.837222 -2.890536 -1.271165  0.351642   \n",
       "1  0.548922  0.104170  0.247237  0.384087 -1.878805 -1.581414  1.528581   \n",
       "2 -0.613926 -0.435071 -0.327684  2.248206 -0.140331 -0.281201  0.297878   \n",
       "\n",
       "    Store_7   Store_8   Store_9  ...   Month_3   Month_4   Month_5    Year_0  \\\n",
       "0  0.352165 -1.067315  0.432503  ...  0.912523  1.552763 -0.231177 -0.952061   \n",
       "1 -0.838187 -1.658705 -2.186104  ...  0.912523  1.552763 -0.231177 -0.952061   \n",
       "2  2.848119  1.672459  1.167650  ...  0.912523  1.552763 -0.231177 -0.952061   \n",
       "\n",
       "     Year_1   Promo_0  StateHoliday_0  StateHoliday_1  StateHoliday_2  \\\n",
       "0  0.123717  0.058928        0.693458       -1.862958       -0.175023   \n",
       "1  0.123717  0.058928        0.693458       -1.862958       -0.175023   \n",
       "2  0.123717  0.058928        0.693458       -1.862958       -0.175023   \n",
       "\n",
       "   SchoolHoliday_0  \n",
       "0        -1.131267  \n",
       "1        -1.131267  \n",
       "2        -1.131267  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_train_xs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de53bb",
   "metadata": {},
   "source": [
    "We can see that our initial dataset, which had 8 columns representing categorical variables, now reaches 39 columns. This number amounts to the *sum of the embedding sizes* we have chosen.\n",
    "\n",
    "We can now retrain our initial model, i.e. the Random Forests, using these new features created by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_m = rf(emb_train_xs.values, train_y.values, max_features=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2101c4",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "Since we now have more columns, the model will take longer to train. We therefore set the parameter `max_features` to 0.6, which defines how many columns to sample at each split point (i.e. 60% of the total).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11000847",
   "metadata": {},
   "source": [
    "Finally, the moment we have all been anxiously waiting for has arrived. Will our model have benefited from the use of Entity Embeddings? ü•Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid RMSPE: 0.139\n",
      "Valid MAPE:  0.101\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "emb_valid_xs, valid_y_clean = drop_zero_sales(emb_valid_xs, valid_y)\n",
    "\n",
    "print(f\"Valid RMSPE: {m_rmspe(emb_m, emb_valid_xs.values, valid_y_clean.values):.3f}\")\n",
    "print(f\"Valid MAPE:  {m_mape(emb_m, emb_valid_xs.values, valid_y_clean.values):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7fe22",
   "metadata": {},
   "source": [
    "That's definitely the case! We started from an RMSPE of about 0.31 using ordinal encoding to arrive at 0.14 with Entity Embeddings&mdash;we've *halved* the error! The score on the *private test* set confirms the results and the positive trend, bringing the RSMPE down to 0.142."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff05ceb",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Similarly to word embeddings in NLP (e.g. word2vec or GloVe), entity embeddings offer a low-dimensional, continuous representation of categorical variables that captures their semantic relationships. We have seen that, although they are derived from the training of a neural network, entity embeddings can also be used by non-deep learning models, such as random forests.\n",
    "\n",
    "In this blog post, we discussed in detail how to construct embeddings and use them as features for an ensemble of decision trees. Along the way, we were able to reproduce part of the results contained in the paper of Guo and Berkhahn. In particular, replacing the ordinal encoded features in our random forests model with entity embeddings resulted in halving the error on the test set.\n",
    "\n",
    "To wrap up, when should you give them a try? They are especially useful for datasets with features that have very high cardinality, where other methods often tend to overfit. Is the art of feature engineering dead? Certainly not. In particular, if you have domain knowledge, it would be a shame not to give it to your model via well-designed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68a02b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
