<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Luca Papariello">
<meta name="dcterms.date" content="2023-09-19">
<meta name="description" content="Bringing the power of neural nets to tree-based models.">

<title>Luca’s Blog - Entity embeddings of categorical variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Luca’s Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/LucaPapariello"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/LPapariello"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Entity embeddings of categorical variables</h1>
                  <div>
        <div class="description">
          Bringing the power of neural nets to tree-based models.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">regression</div>
                <div class="quarto-category">random forests</div>
                <div class="quarto-category">embeddings</div>
                <div class="quarto-category">pytorch</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Luca Papariello </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 19, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a>
  <ul class="collapse">
  <li><a href="#trainvalid-split" id="toc-trainvalid-split" class="nav-link" data-scroll-target="#trainvalid-split">Train/Valid split</a></li>
  <li><a href="#ordinal-encoding" id="toc-ordinal-encoding" class="nav-link" data-scroll-target="#ordinal-encoding">Ordinal encoding</a></li>
  </ul></li>
  <li><a href="#evaluation-metric" id="toc-evaluation-metric" class="nav-link" data-scroll-target="#evaluation-metric">Evaluation metric</a></li>
  <li><a href="#baseline-model" id="toc-baseline-model" class="nav-link" data-scroll-target="#baseline-model">Baseline model</a></li>
  <li><a href="#entity-embeddings" id="toc-entity-embeddings" class="nav-link" data-scroll-target="#entity-embeddings">Entity embeddings</a>
  <ul class="collapse">
  <li><a href="#training-the-embeddings" id="toc-training-the-embeddings" class="nav-link" data-scroll-target="#training-the-embeddings">Training the embeddings</a></li>
  <li><a href="#random-forests-with-entity-embeddings" id="toc-random-forests-with-entity-embeddings" class="nav-link" data-scroll-target="#random-forests-with-entity-embeddings">Random Forests with Entity Embeddings</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-right">
<figure class="figure">
<p><a href="https://www.kaggle.com/code/lucapapariello/entity-embeddings-of-categorical-variables/notebook"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Despite the buzz around generative AI, most applications in industry still originate from tabular datasets. In tabular data, some columns may represent numerical variables, like <em>atmospheric pressure</em>, while others may be categorical variables, like <em>sex</em> or <em>product categories</em>. These can take only a limited number of values. The path to using numerical variables is relatively smooth and requires little preprocessing (for some algorithms, even none at all). In contrast, categorical variables must first be converted into numbers, as this is what a computer can process, and this can be done in many ways.</p>
<p>Two conventional approaches to encode categorical variables are <strong>ordinal encoding</strong> and <strong>one-hot encoding</strong>. Ordinal encoding assigns each unique value to a different integer and, as such, assumes an ordering of the categories. For instance, “Size” could be the name of a categorical columns with values: <em>small</em> &lt; <em>medium</em> &lt; <em>large</em>, which would be mapped to 0, 1, and 2, respectively. One-hot encoding creates instead a new column for each possible value in the original data indicating its presence or absence. If the first sample in the dataset were <em>large</em>, we would get the following one-hot representation: (0, 0, 1). Unlike ordinal encoding, one-hot encoding does not assume an ordering of the categories.</p>
<p>However, both ordinal and one-hot encodings have their limitations. To name a few, in ordinal encoding, the assigned numerical values may introduce unintended relationships or orders between categories that do <em>not</em> exist in the original data. One-hot encoding, while solving the issue of introducing unintended order, can lead to high-dimensional and sparse representations for features with a large number of unique categories. This translates into increased computational complexity and memory usage.</p>
<p>In 2015, the <a href="https://www.kaggle.com/c/rossmann-store-sales">Rossmann sales competition</a> took place on Kaggle. The solution of one of the gold medal winners clearly diverged from the others by using a deep learning model, in one of the first known examples of a cutting-edge deep learning model for tabular data. Rather than using traditional encoding methods, the authors introduced the concept of <strong>Entity Embeddings</strong>. Entity Embeddings provide a way to represent categorical variables as <em>low-dimensional continuous</em> vectors, capturing the underlying semantic relationships between categories. This approach eliminates the limitations of ordinal encoding’s introduced order and one-hot encoding’s curse of dimensionality. Their approach is summarised in the paper <a href="https://arxiv.org/abs/1604.06737">Entity Embeddings of Categorical Variables</a>, by Cheng Guo and Felix Berkhahn.</p>
<p>We’ll explore here the concepts and advantages of Entity Embeddings, diving deep and trying to replicate the main findings of the paper. Along the way, we’ll offer a comprehensive understanding of their applications in general machine learning models.</p>
</section>
<section id="dataset" class="level1">
<h1>Dataset</h1>
<p>We use the very same dataset of the Rossmann sales competition that was used by the authors of the paper—this is readily available on Kaggle. The dataset consists of two parts. The first one is <code>train.csv</code> and comprises of daily sales data for several different stores, while the second one is <code>store.csv</code> and provides additional details about each of these stores. Since the focus here is not on obtaining the best possible result, but on the representation of features and the introduction of Entity Embeddings, we’ll restrict our attention to the base features provided in the first file. Here is a snippet of this dataset:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> path<span class="op">/</span><span class="st">'train.csv'</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(train_data, parse_dates<span class="op">=</span>[<span class="st">'Date'</span>], low_memory<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>train_df.head(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Store</th>
      <th>DayOfWeek</th>
      <th>Date</th>
      <th>Sales</th>
      <th>Customers</th>
      <th>Open</th>
      <th>Promo</th>
      <th>StateHoliday</th>
      <th>SchoolHoliday</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>5</td>
      <td>2015-07-31</td>
      <td>5263</td>
      <td>555</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>5</td>
      <td>2015-07-31</td>
      <td>6064</td>
      <td>625</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>5</td>
      <td>2015-07-31</td>
      <td>8314</td>
      <td>821</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Most of the fields are self-explanatory and their description can be found on the competition webpage. What we need to know here is that <code>Sales</code> is our <em>target variable</em> and represents the turnover on a given day. Apart from the column <code>Date</code>, which is a type in its own right, and <code>Customers</code>, which is not available at test time and will hence not be considered, all the features of this dataset are categorical. It should be noted that while most of them have a low cardinality, <code>Store</code>, which is a unique ID identifying each shop, can take a whopping 1115 different values!</p>
<p>Before moving on, we get rid of closed stores, which have zero sales, as we’ll not make any predictions on them at test time.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>to_keep <span class="op">=</span> <span class="op">~</span>((train_df.Open <span class="op">==</span> <span class="dv">0</span>) <span class="op">&amp;</span> (train_df.Sales <span class="op">==</span> <span class="dv">0</span>))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> train_df.loc[to_keep, :]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="trainvalid-split" class="level2">
<h2 class="anchored" data-anchor-id="trainvalid-split">Train/Valid split</h2>
<p>Our training data spans approximately 2.5 years from 2013 to mid-2015. The test data covers instead the subsequent part of 2015, with no overlap with the training set dates. To test the generalisation capability of our model, we’ll try to put ourselves in the closest possible situation to this by sorting the training data by <code>Date</code> and keeping the last 10% of the samples for validation. In this way, we use the older samples for training and the most recent ones for validation. This should ensure that the performance observed on the validation set is as close as possible to that of the leaderboard after submission.</p>
<p>We’ll also drop samples from the validation set for which there are zero sales. This is to align ourselves with the competition, whose website states that “Any day and store with 0 sales is ignored in scoring.”.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>tgt <span class="op">=</span> <span class="st">'Sales'</span>  <span class="co"># Target variable</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> train_df.sort_values(by<span class="op">=</span>[<span class="st">'Date'</span>]).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>train_xs, valid_xs, train_y, valid_y <span class="op">=</span> train_test_split(</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    train_df.drop(columns<span class="op">=</span>tgt), train_df[tgt], test_size<span class="op">=</span><span class="fl">0.1</span>, shuffle<span class="op">=</span><span class="va">False</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>valid_xs, valid_y <span class="op">=</span> drop_zero_sales(valid_xs, valid_y)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Train size: </span><span class="sc">{</span><span class="bu">len</span>(train_y)<span class="sc">:&gt;6}</span><span class="ss">'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Valid size: </span><span class="sc">{</span><span class="bu">len</span>(valid_y)<span class="sc">:&gt;6}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Train size: 759952
Valid size:  84439</code></pre>
</div>
</div>
</section>
<section id="ordinal-encoding" class="level2">
<h2 class="anchored" data-anchor-id="ordinal-encoding">Ordinal encoding</h2>
<p>As a first (and only) operation to prepare the data, we enrich the representation of dates. We keep it simple and only create new columns for <code>Day</code>, <code>Month</code>, and <code>Year</code> out of <code>Date</code>. After all, there are already the <code>DayOfWeek</code>, <code>StateHoliday</code>, and <code>SchoolHoliday</code> columns providing additional information in this regard.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> proc_data(df: pd.DataFrame):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Process DataFrame and create date features inplace."</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'Day'</span>] <span class="op">=</span> df.Date.dt.day</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'Month'</span>] <span class="op">=</span> df.Date.dt.month</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'Year'</span>] <span class="op">=</span> df.Date.dt.year</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'Open'</span>] <span class="op">=</span> df.Open.fillna(<span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>proc_data(train_xs)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>proc_data(valid_xs)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>proc_data(test_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We’re now ready to map our categorical variables to integers using scikit-learn’s <code>OrdinalEncoder</code>. In addition, we only select the relevant features.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>cats <span class="op">=</span> [<span class="st">'Store'</span>, <span class="st">'DayOfWeek'</span>, <span class="st">'Day'</span>, <span class="st">'Month'</span>, <span class="st">'Year'</span>, <span class="st">'Promo'</span>, <span class="st">'StateHoliday'</span>, <span class="st">'SchoolHoliday'</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>oe <span class="op">=</span> OrdinalEncoder(dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>train_xs[cats] <span class="op">=</span> oe.fit_transform(train_xs[cats])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>valid_xs[cats] <span class="op">=</span> oe.transform(valid_xs[cats])</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>test_df[cats] <span class="op">=</span> oe.transform(test_df[cats])</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>train_xs, valid_xs, test_xs <span class="op">=</span> train_xs[cats], valid_xs[cats], test_df[cats]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s have a look at our preprocessed dataframe, which is now ready to be passed to our first model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>train_xs.tail(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Store</th>
      <th>DayOfWeek</th>
      <th>Day</th>
      <th>Month</th>
      <th>Year</th>
      <th>Promo</th>
      <th>StateHoliday</th>
      <th>SchoolHoliday</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>759949</th>
      <td>351</td>
      <td>5</td>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>759950</th>
      <td>350</td>
      <td>5</td>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>759951</th>
      <td>364</td>
      <td>5</td>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
</section>
<section id="evaluation-metric" class="level1">
<h1>Evaluation metric</h1>
<p>Before moving on to the model, we want to take a look at one last important piece: the <em>evaluation metric</em>. Submissions for this Kaggle competitions are evaluated in terms of <em>Root Mean Square Percentage Error</em> (RMSPE), so we’ll use the same metric here. Additionally, we pick another common evaluation metric, the <em>Mean Absolute Percentage Error</em> (MAPE). This is the metric used in the publication and will allow us to benchmark our results against theirs.</p>
</section>
<section id="baseline-model" class="level1">
<h1>Baseline model</h1>
<p>We choose Random Forests as our starting point, which is an ensemble of decision trees. Reason for this is that it requires little preprocessing, it isn’t very sensitive to hyperparameters, and generally provides a strong baseline.</p>
<p>For convenience, we define an <code>rf</code> function that returns a <em>fitted</em> Random Forests regression model. We are now ready to fit our first model and check its performance, which is expressed in terms of RMSPE (the competition metric) and MAPE (the metric used in the paper).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rf(xs, y, n_estimators<span class="op">=</span><span class="dv">30</span>, max_samples<span class="op">=</span><span class="dv">200_000</span>, max_depth<span class="op">=</span><span class="dv">35</span>, min_samples_leaf<span class="op">=</span><span class="dv">5</span>, <span class="op">**</span>kwargs):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Return a fitted Random Forests regression model."</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> RandomForestRegressor(</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span>, n_estimators<span class="op">=</span>n_estimators, max_samples<span class="op">=</span>max_samples,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span>max_depth, min_samples_leaf<span class="op">=</span>min_samples_leaf, random_state<span class="op">=</span><span class="dv">34</span>, <span class="op">**</span>kwargs</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    ).fit(xs, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> rf(train_xs.values, train_y.values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Valid RMSPE: </span><span class="sc">{</span>m_rmspe(m, valid_xs.values, valid_y.values)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Valid MAPE:  </span><span class="sc">{</span>m_mape(m, valid_xs.values, valid_y.values)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Valid RMSPE: 0.310
Valid MAPE:  0.185</code></pre>
</div>
</div>
<p>Submitting the results of this model would result in a score on the <em>private test</em> set of about 0.303—note how close this is to the score on our validation set! This is good news because it suggests that we can trust our validation set. However, this result would put us a long way from the top of the leaderboard, where the winner had an impressive score of 0.1.</p>
</section>
<section id="entity-embeddings" class="level1">
<h1>Entity embeddings</h1>
<p>Embeddings are at the heart of many recommender systems, particularly in cases where the possibility of employing a content-based approach is ruled out due to the lack of information about users and items. Collaborative filtering offers a (somewhat surprising) solution for predicting user preferences based only on the interests of other users. The key idea here is that of <strong>latent dimensions</strong>, which are features that describe users and items and are <em>automatically</em> discovered by a model. The resulting matrices containing the latent factors of users and items are exactly the user and item embedding matrices. Typically, they are determined via <em>low-rank matrix factorisation</em> or randomly initialised and improved during the <em>training of a neural network</em>.</p>
<p>The entity embeddings we discuss today generalise this same idea to any categorical variable, i.e.&nbsp;we want to learn a low-dimensional vector representation for each category in a given categorical feature. We’ll optimise these embeddings during the training process along with the rest of the model’s parameters. The underlying intuition is that by representing categories as continuous vectors, the model can learn meaningful relationships and similarities between categories through their proximity in the embedding space. This allows the model to capture complex interactions and dependencies among categorical variables, which can be very valuable for predictive tasks.</p>
<section id="training-the-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="training-the-embeddings">Training the embeddings</h2>
<p>The deep learning framework we will use to build and train our neural network is PyTorch. In PyTorch, a Dataset is constructed by subclassing <code>Dataset</code> and requires us to override two dunder methods: <code>__getitem__</code> and <code>__len__</code>. In the following we implement our Dataset to return a tensor for the features and one for the target for the training and validation set, whereas only the features for the test set.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TabDataset(Dataset):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Tabular Dataset that yields categorical features and target."</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, feats, new_data<span class="op">=</span><span class="va">True</span>, tgt<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x_cat <span class="op">=</span> torch.tensor(feats.values, dtype<span class="op">=</span>torch.int32)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_samples <span class="op">=</span> <span class="bu">len</span>(feats)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.new_data <span class="op">=</span> new_data</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> new_data:</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.y <span class="op">=</span> torch.tensor(tgt.values, dtype<span class="op">=</span>torch.float32).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.n_samples</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx: <span class="bu">int</span>):</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.new_data:</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.x_cat[idx], <span class="va">self</span>.y[idx]</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.x_cat[idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> TabDataset(train_xs, new_data<span class="op">=</span><span class="va">False</span>, tgt<span class="op">=</span>train_y)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>valid_ds <span class="op">=</span> TabDataset(valid_xs, new_data<span class="op">=</span><span class="va">False</span>, tgt<span class="op">=</span>valid_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we can build the training and validation <code>DataLoader</code>s, which take a Dataset and return an iterable that handles shuffling, batching, and all the rest for us.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">1024</span>  <span class="co"># Batch size</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> DataLoader(train_ds, batch_size<span class="op">=</span>bs, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>valid_dl <span class="op">=</span> DataLoader(valid_ds, batch_size<span class="op">=</span>bs<span class="op">*</span><span class="dv">2</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now comes the main part, the construction of the neural network that will learn the entity embeddings. In PyTorch, we can easily create custom models by subclassing the <code>nn.Module</code> module. In a nutshell, this neural network consists of an initial part of embedding layers—these represent our matrices of latent factors—followed by a fully connected part.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TabularNeuralNet(nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Neural network model for tabular data."</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        emb_szs: <span class="bu">list</span>,  <span class="co"># List of (unique_cats, embedding_dim)</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        out_sz: <span class="bu">int</span>,    <span class="co"># Number of outputs for final layer</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        layers: <span class="bu">list</span>,   <span class="co"># Size of the hidden layers</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        act_cls<span class="op">=</span>nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),  <span class="co"># Activation type after `Linear` layers</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeds <span class="op">=</span> nn.ModuleList([nn.Embedding(ni, nf) <span class="cf">for</span> ni, nf <span class="kw">in</span> emb_szs])</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        n_emb <span class="op">=</span> <span class="bu">sum</span>(e.embedding_dim <span class="cf">for</span> e <span class="kw">in</span> <span class="va">self</span>.embeds)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_emb <span class="op">=</span> n_emb</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        sizes <span class="op">=</span> [n_emb] <span class="op">+</span> layers <span class="op">+</span> [out_sz]</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        actns <span class="op">=</span> [act_cls <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sizes) <span class="op">-</span> <span class="dv">2</span>)] <span class="op">+</span> [<span class="va">None</span>]</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        _layers <span class="op">=</span> [LinReLu(sizes[i], sizes[i <span class="op">+</span> <span class="dv">1</span>], act<span class="op">=</span>a) <span class="cf">for</span> i, a <span class="kw">in</span> <span class="bu">enumerate</span>(actns)]</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(<span class="op">*</span>_layers)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x_cat):</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> [e(x_cat[:, i]) <span class="cf">for</span> i, e <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.embeds)]</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat(x, <span class="dv">1</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layers(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The only thing that remains for us to do is to determine the <code>emb_szs</code> parameter. The part concerning the number of unique categories can easily be found using the <code>OrdinalEncoder</code> we fitted above, but the embedding size for each categorical feature is a free parameter. As such, there is no clear-cut way to determine it but it’s typically <em>much smaller</em> than the number of unique categories. We are thus thankful that the authors report these values in their paper.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>num_unique <span class="op">=</span> [<span class="bu">len</span>(c) <span class="cf">for</span> c <span class="kw">in</span> oe.categories_]</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>emb_dims <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">6</span>, <span class="dv">10</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>embed_sizes <span class="op">=</span> [(u, e) <span class="cf">for</span> u, e <span class="kw">in</span> <span class="bu">zip</span>(num_unique, emb_dims)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>nn_model <span class="op">=</span> TabularNeuralNet(emb_szs<span class="op">=</span>embed_sizes, out_sz<span class="op">=</span><span class="dv">1</span>, layers<span class="op">=</span>[<span class="dv">200</span>, <span class="dv">100</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>What follows is the classic PyTorch training loop, which we encapsulate for convenience in two functions, <code>train_one_epoch</code> and <code>validate_one_epoch</code>, that do what they say: train and validate the model over one epoch, respectively. We train our model for a few epochs using the standard Mean Squared Error (MSE) loss function and Adam optimiser.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ep <span class="kw">in</span> <span class="bu">range</span>(EPOCHS):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>ep<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">31</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    train_one_epoch(train_dl, nn_model, loss_func, optim)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    validate_one_epoch(valid_dl, nn_model, loss_func)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1
-------------------------------
Loss: 56439536  [     0/759952]
Loss:  1829678  [409600/759952]
Loss:  2018285  [759952/759952]
Avg. valid. loss: 1823780, Avg. RMSPE: 0.2195

Epoch 2
-------------------------------
Loss:  1885142  [     0/759952]
Loss:  1538159  [409600/759952]
Loss:  1631149  [759952/759952]
Avg. valid. loss: 1681688, Avg. RMSPE: 0.2059

Epoch 3
-------------------------------
Loss:  1414050  [     0/759952]
Loss:  1237316  [409600/759952]
Loss:   899826  [759952/759952]
Avg. valid. loss: 1374336, Avg. RMSPE: 0.1720
</code></pre>
</div>
</div>
<p>That’s it! By training our neural network we obtained the entity embeddings. As a nice side-product we also got a fully functioning model that we can use to forecast the sales.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Valid RMSPE: </span><span class="sc">{</span>rmspe(all_preds, valid_y.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Valid MAPE:  </span><span class="sc">{</span>mape(all_preds, valid_y.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Valid RMSPE: 0.179
Valid MAPE:  0.123</code></pre>
</div>
</div>
<p>It turns out that this model works much better than our previous baseline, lowering both RMSPE and MAPE substantially. Submitting these results would bring the score on the <em>private test</em> set down to about 0.17. We are definitely moving in the right direction!</p>
</section>
<section id="random-forests-with-entity-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="random-forests-with-entity-embeddings">Random Forests with Entity Embeddings</h2>
<p>Now that we have trained the neural network, we have the entity embeddings at our disposal. All that remains is to extract them from the embedding layer of our <code>TabularNeuralNet</code>. We write a function, <code>embed_features</code>, to do that since we’ll have to repeat this operation for the training, validation and test set<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> embed_features(model, xs, encoder):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Replace categorical columns in `xs` w/ embeddings extracted from `model`."</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> xs.copy()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, col <span class="kw">in</span> <span class="bu">enumerate</span>(encoder.feature_names_in_):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get embedding matrix</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>            emb <span class="op">=</span> model.embeds[i]</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>            emb_data <span class="op">=</span> emb(torch.tensor(xs[col].values, dtype<span class="op">=</span>torch.int32))</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>            emb_names <span class="op">=</span> [<span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(emb_data.shape[<span class="dv">1</span>])]</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Replace old feature col. w/ new one(s)</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>            feat_df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>                data<span class="op">=</span>emb_data.cpu().numpy(), index<span class="op">=</span>xs.index, columns<span class="op">=</span>emb_names</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>            xs <span class="op">=</span> xs.drop(col, axis<span class="op">=</span><span class="dv">1</span>).join(feat_df)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> xs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We are now ready to apply the mapping defined by the embeddings to our original features. We also show the first few rows to see what the result looks like.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train embeddings</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>emb_train_xs <span class="op">=</span> embed_features(nn_model, train_xs, oe)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Validation embeddings</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>emb_valid_xs <span class="op">=</span> embed_features(nn_model, valid_xs, oe)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>emb_train_xs.head(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Store_0</th>
      <th>Store_1</th>
      <th>Store_2</th>
      <th>Store_3</th>
      <th>Store_4</th>
      <th>Store_5</th>
      <th>Store_6</th>
      <th>Store_7</th>
      <th>Store_8</th>
      <th>Store_9</th>
      <th>...</th>
      <th>Month_3</th>
      <th>Month_4</th>
      <th>Month_5</th>
      <th>Year_0</th>
      <th>Year_1</th>
      <th>Promo_0</th>
      <th>StateHoliday_0</th>
      <th>StateHoliday_1</th>
      <th>StateHoliday_2</th>
      <th>SchoolHoliday_0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.464628</td>
      <td>1.389546</td>
      <td>0.031453</td>
      <td>0.837222</td>
      <td>-2.890536</td>
      <td>-1.271165</td>
      <td>0.351642</td>
      <td>0.352165</td>
      <td>-1.067315</td>
      <td>0.432503</td>
      <td>...</td>
      <td>0.912523</td>
      <td>1.552763</td>
      <td>-0.231177</td>
      <td>-0.952061</td>
      <td>0.123717</td>
      <td>0.058928</td>
      <td>0.693458</td>
      <td>-1.862958</td>
      <td>-0.175023</td>
      <td>-1.131267</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.548922</td>
      <td>0.104170</td>
      <td>0.247237</td>
      <td>0.384087</td>
      <td>-1.878805</td>
      <td>-1.581414</td>
      <td>1.528581</td>
      <td>-0.838187</td>
      <td>-1.658705</td>
      <td>-2.186104</td>
      <td>...</td>
      <td>0.912523</td>
      <td>1.552763</td>
      <td>-0.231177</td>
      <td>-0.952061</td>
      <td>0.123717</td>
      <td>0.058928</td>
      <td>0.693458</td>
      <td>-1.862958</td>
      <td>-0.175023</td>
      <td>-1.131267</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.613926</td>
      <td>-0.435071</td>
      <td>-0.327684</td>
      <td>2.248206</td>
      <td>-0.140331</td>
      <td>-0.281201</td>
      <td>0.297878</td>
      <td>2.848119</td>
      <td>1.672459</td>
      <td>1.167650</td>
      <td>...</td>
      <td>0.912523</td>
      <td>1.552763</td>
      <td>-0.231177</td>
      <td>-0.952061</td>
      <td>0.123717</td>
      <td>0.058928</td>
      <td>0.693458</td>
      <td>-1.862958</td>
      <td>-0.175023</td>
      <td>-1.131267</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 39 columns</p>
</div>
</div>
</div>
<p>We can see that our initial dataset, which had 8 columns representing categorical variables, now reaches 39 columns. This number amounts to the <em>sum of the embedding sizes</em> we have chosen.</p>
<p>We can now retrain our initial model, i.e.&nbsp;the Random Forests, using these new features created by the neural network.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>emb_m <span class="op">=</span> rf(emb_train_xs.values, train_y.values, max_features<span class="op">=</span><span class="fl">0.6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Since we now have more columns, the model will take longer to train. We therefore set the parameter <code>max_features</code> to 0.6, which defines how many columns to sample at each split point (i.e.&nbsp;60% of the total).</p>
</div>
</div>
<p>Finally, the moment we have all been anxiously waiting for has arrived. Will our model have benefited from the use of Entity Embeddings? 🥁</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>emb_valid_xs, valid_y_clean <span class="op">=</span> drop_zero_sales(emb_valid_xs, valid_y)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Valid RMSPE: </span><span class="sc">{</span>m_rmspe(emb_m, emb_valid_xs.values, valid_y_clean.values)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Valid MAPE:  </span><span class="sc">{</span>m_mape(emb_m, emb_valid_xs.values, valid_y_clean.values)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Valid RMSPE: 0.139
Valid MAPE:  0.101</code></pre>
</div>
</div>
<p>That’s definitely the case! We started from an RMSPE of about 0.31 using ordinal encoding to arrive at 0.14 with Entity Embeddings—we’ve <em>halved</em> the error! The score on the <em>private test</em> set confirms the results and the positive trend, bringing the RSMPE down to 0.142.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Similarly to word embeddings in NLP (e.g.&nbsp;word2vec or GloVe), entity embeddings offer a low-dimensional, continuous representation of categorical variables that captures their semantic relationships. We have seen that, although they are derived from the training of a neural network, entity embeddings can also be used by non-deep learning models, such as random forests.</p>
<p>In this blog post, we discussed in detail how to construct embeddings and use them as features for an ensemble of decision trees. Along the way, we were able to reproduce part of the results contained in the paper of Guo and Berkhahn. In particular, replacing the ordinal encoded features in our random forests model with entity embeddings resulted in halving the error on the test set.</p>
<p>To wrap up, when should you give them a try? They are especially useful for datasets with features that have very high cardinality, where other methods often tend to overfit. Is the art of feature engineering dead? Certainly not. In particular, if you have domain knowledge, it would be a shame not to give it to your model via well-designed features.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Note that our main evaluation metric, the RMSPE, would diverge when the target is zero.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>If you come from the fastai world, check out <a href="https://towardsdatascience.com/entity-embeddings-for-ml-2387eb68e49">this blog</a> on Medium where this process is done using fastai objects (TabularPandas, Learner, etc.).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>