{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Gensim: share vocabulary across models\"\n",
    "date: \"2021-07-27\"\n",
    "author: Luca Papariello\n",
    "categories:\n",
    "- nlp\n",
    "- gensim\n",
    "description: A brief illustration of how to share a common vocabulary among different\n",
    "  Gensim models.\n",
    "toc: true\n",
    "image: fig_bots_dict.jpg\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://kaggle.com/static/images/open-in-kaggle.svg){fig-align=\"right\"}](https://www.kaggle.com/code/lucapapariello/gensim-share-vocabulary-across-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Deep learning models based on the transformer architecture have taken the NLP world by storm in the last few years, achieving state-of-the-art results in several areas. An obvious example of this success is provided by the tremendous growth of the [Hugging Face](https://huggingface.co/) ecosystem, which provides access to a plethora of pre-trained models in a very user-friendly way. \n",
    "\n",
    "However, we believe that models based on (static) word embeddings still have their place in the \"transformer era\". Some reasons why this might be the case are the following: \n",
    "\n",
    "- Transformer-based models are usually much bigger (i.e. more parameters) than \"standard\" models.\n",
    "- Transformer models are not renowned for their (inference) speed&mdash;this is related to the previous point.\n",
    "- Models based on word embeddings still provide a solid baseline.\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/) is a great library when it comes to word embeddings, and some other NLP tasks, especially if you want to train them on your own. There might be cases where you would like to train two NLP models and have them \"speak the same language\", i.e. share the *same* vocabulary. For the sake of concreteness, let's say these two models are [LSI](https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing) and [word2vec](https://en.wikipedia.org/wiki/Word2vec). The \"standard\" way of doing this, however, requires the preparation of *two* vocabularies, one for each model. In this post, we'll show how to avoid this by transferring the vocabuly of the LSI model to the word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI and word2vec the \"standard\" way\n",
    "\n",
    "We will now build these two models following the \"standard\" procedure as can be found in the respective Gensim documentation. In what follows, we will work with the [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/), which is a collection of ca. 20,000 newsgroup documents grouped into 20 classes. Details are not very important in relation to our discussion. This dataset can easily be downloaded using the `sklearn.datasets.fetch_20newsgroups` [function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) of the scikit-learn libray, which will download and cache the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "if IN_COLAB:\n",
    "    !pip install nltk==3.2.4 -q\n",
    "    !pip install scikit-learn==0.23.2 -q\n",
    "    !pip install gensim==4.0.1 -q\n",
    "    !pip install smart-open==5.1.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luca/anaconda3/envs/torch/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "import re\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, LsiModel, Word2Vec\n",
    "from smart_open import open as sopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "data, _ = fetch_20newsgroups(\n",
    "    shuffle=True, random_state=123, remove=('headers', 'footers', 'quotes'), return_X_y=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def clean_text(txt: str):\n",
    "    '''Clean and lower case text.'''\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
    "    txt = re.sub(r\"\\b\\d+\\b\", \"\", txt).strip()\n",
    "    return txt\n",
    "\n",
    "\n",
    "def tokenizer(txt: str):\n",
    "    '''Custom tokenizer.'''\n",
    "    tokens = []\n",
    "    \n",
    "    # split strings into sentences\n",
    "    for sent in sent_tokenize(txt, language='english'):\n",
    "        # split each sentence into tokens and apply text cleaning \n",
    "        for word in word_tokenize(clean_text(sent), language='english'):\n",
    "            if len(word) < 2:  # remove short words\n",
    "                continue\n",
    "            if word in stop_words:  # remove stop words\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI model\n",
    "\n",
    "The first step in [building an LSI model](https://radimrehurek.com/gensim/models/lsimodel.html) is to create a dictionary, which maps words to integer ids. This is easily achieved through the `Dictionary` class, to which we have to pass tokenised documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [tokenizer(doc) for doc in data]\n",
    "dct = Dictionary(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Remove words that appear less than 2 times\n",
    "rare_ids = [tokenid for tokenid, wordfreq in dct.cfs.items() if wordfreq < 2]\n",
    "# Drop tokens\n",
    "dct.filter_tokens(rare_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the dictionary we can then build our corpus using the `.doc2bow()` method. This returns documents in a bag-of-words (BoW) representation. We could proceed with it, but a [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) representation is preferable, for which we can use the `TfidfModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dct.doc2bow(line) for line in tokenized_data]\n",
    "tfidf_model = TfidfModel(corpus, id2word=dct)\n",
    "tfidf_matrix = tfidf_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need to build our LSI model, which is conveniently done by the `LsiModel` class. Without further motivating this arbitrary choice, we set the number of latent dimensions to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 625 ms, total: 13.2 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dim_lsi = 200  # Topic number (latent dimension)\n",
    "lsi_model = LsiModel(corpus=tfidf_matrix, id2word=dct, num_topics=dim_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an LSI model ready to be used! Let's move on to word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec model\n",
    "\n",
    "The quickest way to train a [word2vec model](https://radimrehurek.com/gensim/models/word2vec.html) is through the `Word2Vec` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "dim_w2v = dim_lsi  # Diminsionality of word vectors\n",
    "alpha = 0.025  # Initial learning rate\n",
    "alpha_min = 0.0001  # Drop learning rate to this value\n",
    "wnd = 5        # Window size (max. distance to predicted word)\n",
    "mincount = 2   # Word frequency lower bound\n",
    "sample = 1e-5  # Threshold for downsampling\n",
    "sg = 1         # Index 1 => Skip-Gram algo.\n",
    "ngt = 10       # No. noisy words for negative sampling\n",
    "epochs = 5     # No. epochs for training\n",
    "cpus = multiprocessing.cpu_count()  # Tot. no. of CPUs\n",
    "threads = cpus -1  # Use this number of threads for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 1.19 s, total: 1min 32s\n",
      "Wall time: 53.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_data, vector_size=dim_w2v, alpha=alpha, min_alpha=alpha_min, window=wnd, \n",
    "    min_count=mincount, sample=sample, sg=sg, negative=ngt, epochs=epochs, workers=threads\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check the number of words present in each of our two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of LSI vocab.: 42439\n",
      "Size of w2v vocab.: 42439\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "print('Size of LSI vocab.:', len(dct.keys()))\n",
    "print('Size of w2v vocab.:', len(w2v_model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve hence managed to build an LSI and a word2vec model whose vocabularies contain the exact same words&mdash;great!  However, this came at an unnecessarily high price and we'll shortly see why. What happens behind the scenes when we create a new instance of the `Word2Vec` class is the following. First a quick sanity check of the corpus is performed, then the vocabulary is built using the `.build_vocab()` method, and lastly the method `.train()` is executed, which trains the model. In the second step, a new dictionary is built from scratch, despite having already done so for the LSI model. When working with small datasets this procedure might be acceptable, but when the corpus is very large optimising this step can save a lot of time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI and word2vec the fast way\n",
    "\n",
    "We will now see how we can build these models avoiding the above issue. To do that, we must split the creation of the model into three steps. We start by instantiating the model, but without passing it a corpus, i.e. leaving it uninitialised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    vector_size=dim_w2v, alpha=alpha, min_alpha=alpha_min, window=wnd, \n",
    "    min_count=mincount, sample=sample, sg=sg, negative=ngt, workers=threads\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, for the second step, Gensim offers an easy workaround: one can build a vocabulary from a dictionary of *word frequencies*, instead of from a sequence of sentences as done by default by `.build_vocab()`. This can be done with the method `.build_vocab_from_freq()`, which requires a frequency mapping. The latter can be obtained from the LSI dictionary, specifically from the `dct.cfs` attribute, which contains index to frequency mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 812 ms, total: 1min 30s\n",
      "Wall time: 37.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2609400, 6277620)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 2: borrow LSI vocab.\n",
    "word_freq = {dct[k]: v for k,v in dct.cfs.items()}\n",
    "w2v_model.build_vocab_from_freq(word_freq)\n",
    "# Step 3: train model\n",
    "num_samples = dct.num_docs\n",
    "w2v_model.train(tokenized_data, total_examples=num_samples, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been successful in creating the word2vec model by borrowing the LSI vocabulary. This allowed us to avoid an unnecessary step and hence to waste resources.\n",
    "\n",
    "Note that in this case the speed-up is barely observable, which is due to the very small size of the dataset (about 15 MB). However, this becomes considerable when working with huge datasets. The dataset on which I base these conclusions exceeds 50 GB and this second approach saved me *several hours*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data streaming [optional]\n",
    "\n",
    "This is a bonus section for those who have endured until this point. The motivation behind this post was to avoid unnecessary calculations, which makes particular sense when dealing with very large datasets. Very large datasets will most likely *not* fit in the memory, but in the above code we have loaded everything into RAM&mdash;ouch!\n",
    "\n",
    "Gensim models are smart enough to accept *iterables* that stream the input data directly from disk. In this way, our corpus can be arbitrarily large (limited only by the size of our hard drive). We repeat here the steps of the above sections, but restructuring our code to take advantage of *data streaming*. We assume the corpus to be stored in a unique text file (`20news.txt`), which we get by writing the `data` list to file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "Loading a corpus into memory and then dumping it into a file obviously doesn't make much sense; we're doing this only to work with the same data as before. You will not need this step as you will be starting directly from some data stored in a (potentially very large) file.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "\n",
    "def write_to_file(txt, out_file):\n",
    "    '''Write text corpus to file (line by line).'''\n",
    "    with open(out_file, 'a') as f:\n",
    "        for line in txt:\n",
    "            f.write(line + '\\n')\n",
    "            \n",
    "file_out = Path('20news.txt')\n",
    "\n",
    "write_to_file(data, file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI model\n",
    "\n",
    "As we've seen before, the first step is to create a dictionary. Before we passed a list to `Dictionary`, now we pass it a *generator*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "curpus_path = Path('20news.txt')\n",
    "dct = Dictionary((tokenizer(line) for line in open(curpus_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Remove words that appear less than 2 times\n",
    "rare_ids = [tokenid for tokenid, wordfreq in dct.cfs.items() if wordfreq < 2]\n",
    "# Drop tokens\n",
    "dct.filter_tokens(rare_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step two consists in creating a corpus and switching to a TF-IDF representation. Here is where things change a bit. We need to define an iterable that yields documents in BoW representation, which is done by the `Corpus` class here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    '''Iterable that yields BoW representations of documents.'''\n",
    "    \n",
    "    def __init__(self, curpus_path, dct_object):\n",
    "        self.curpus_path = curpus_path\n",
    "        self.dct_object = dct_object\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in sopen(self.curpus_path):\n",
    "            yield self.dct_object.doc2bow(tokenizer(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use it to create our streamed corpus, which can be passed to `TfidfModel`. We'll skip the explicit creation of the TF-IDF matrix because it can be very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(curpus_path, dct)\n",
    "tfidf_model = TfidfModel(corpus, id2word=dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build our LSI model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 58s, sys: 11 s, total: 4min 9s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lsi_model = LsiModel(corpus=tfidf_model[corpus], id2word=dct, num_topics=dim_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "We need to use iterables and not generators even though they both produce an iterator. This is because after we have exhausted a generator once there is no more data available. In contrast, iterables create a new iterator *every time* they are looped over. This is exactly what we need when creating a model: we need to be able to iterate over a dataset more than once.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec model\n",
    "\n",
    "Similar to what we did with the LSI model, we need to define an iterable that yields tokenized documents. This is provided by the `CorpusW2V` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusW2V:\n",
    "    '''Iterable that yields sentences (lists of str).'''\n",
    "\n",
    "    def __init__(self, curpus_path):\n",
    "        self.curpus_path = curpus_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in sopen(self.curpus_path):\n",
    "            yield tokenizer(line)\n",
    "\n",
    "corpus_w2v = CorpusW2V(curpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest follows exactly as above, with the only difference that now the `.train()` method receives an instance of the `CorpusW2V` class instead of a list (see `tokenized_data` above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 52s, sys: 4.78 s, total: 5min 56s\n",
      "Wall time: 6min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2608743, 6277620)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    vector_size=dim_w2v, alpha=alpha, min_alpha=alpha_min, window=wnd, \n",
    "    min_count=mincount, sample=sample, sg=sg, negative=ngt, workers=threads\n",
    ")\n",
    "# Borrow LSI vocab.\n",
    "word_freq = {dct[k]: v for k,v in dct.cfs.items()}\n",
    "w2v_model.build_vocab_from_freq(word_freq)\n",
    "# Train model\n",
    "num_samples = dct.num_docs\n",
    "w2v_model.train(corpus_w2v, total_examples=num_samples, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "curpus_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude by noting that this approach based on data streaming is certainly *slower* than when we load everything into memory. However, it allows us to process arbitrarily large datasets. One can't have it all, as they say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements and references\n",
    "\n",
    "We must thank the Gensim community and in particular Austen Mack-Crane, on whose suggestions the section [LSI and word2vec the fast way](#LSI-and-word2vec-the-fast-way) is based. The section [Data streaming](#Data-streaming-[optional]) takes instead inspiration from the [Gensim documentation](https://radimrehurek.com/gensim/) and from Radim Řehůřek's [blog post](https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/) about data streaming in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "533127276ba1cdee321fa1406e72f6fcf4a9b762ca0111986e7d2ff464695919"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
