[
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary/index.html",
    "href": "posts/2021-07-27-gensim-shared-vocabulary/index.html",
    "title": "Gensim: share vocabulary across models",
    "section": "",
    "text": "Deep learning models based on the transformer architecture have taken the NLP world by storm in the last few years, achieving state-of-the-art results in several areas. An obvious example of this success is provided by the tremendous growth of the Hugging Face ecosystem, which provides access to a plethora of pre-trained models in a very user-friendly way.\nHowever, we believe that models based on (static) word embeddings still have their place in the “transformer era”. Some reasons why this might be the case are the following:\n\nTransformer-based models are usually much bigger (i.e. more parameters) than “standard” models.\nTransformer models are not renowned for their (inference) speed—this is related to the previous point.\nModels based on word embeddings still provide a solid baseline.\n\nGensim is a great library when it comes to word embeddings, and some other NLP tasks, especially if you want to train them on your own. There might be cases where you would like to train two NLP models and have them “speak the same language”, i.e. share the same vocabulary. For the sake of concreteness, let’s say these two models are LSI and word2vec. The “standard” way of doing this, however, requires the preparation of two vocabularies, one for each model. In this post, we’ll show how to avoid this by transferring the vocabuly of the LSI model to the word2vec model."
  },
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary/index.html#lsi-model",
    "href": "posts/2021-07-27-gensim-shared-vocabulary/index.html#lsi-model",
    "title": "Gensim: share vocabulary across models",
    "section": "LSI model",
    "text": "LSI model\nThe first step in building an LSI model is to create a dictionary, which maps words to integer ids. This is easily achieved through the Dictionary class, to which we have to pass tokenised documents:\n\ntokenized_data = [tokenizer(doc) for doc in data]\ndct = Dictionary(tokenized_data)\n\nWith the help of the dictionary we can then build our corpus using the .doc2bow() method. This returns documents in a bag-of-words (BoW) representation. We could proceed with it, but a TF-IDF representation is preferable, for which we can use the TfidfModel class.\n\ncorpus = [dct.doc2bow(line) for line in tokenized_data]\ntfidf_model = TfidfModel(corpus, id2word=dct)\ntfidf_matrix = tfidf_model[corpus]\n\nWe have everything we need to build our LSI model, which is conveniently done by the LsiModel class. Without further motivating this arbitrary choice, we set the number of latent dimensions to 200.\n\n%%time\n\ndim_lsi = 200  # Topic number (latent dimension)\nlsi_model = LsiModel(corpus=tfidf_matrix, id2word=dct, num_topics=dim_lsi)\n\nCPU times: user 12.6 s, sys: 625 ms, total: 13.2 s\nWall time: 10.5 s\n\n\nWe now have an LSI model ready to be used! Let’s move on to word2vec."
  },
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary/index.html#word2vec-model",
    "href": "posts/2021-07-27-gensim-shared-vocabulary/index.html#word2vec-model",
    "title": "Gensim: share vocabulary across models",
    "section": "word2vec model",
    "text": "word2vec model\nThe quickest way to train a word2vec model is through the Word2Vec class.\n\n\nCode\ndim_w2v = dim_lsi  # Diminsionality of word vectors\nalpha = 0.025  # Initial learning rate\nalpha_min = 0.0001  # Drop learning rate to this value\nwnd = 5        # Window size (max. distance to predicted word)\nmincount = 2   # Word frequency lower bound\nsample = 1e-5  # Threshold for downsampling\nsg = 1         # Index 1 => Skip-Gram algo.\nngt = 10       # No. noisy words for negative sampling\nepochs = 5     # No. epochs for training\ncpus = multiprocessing.cpu_count()  # Tot. no. of CPUs\nthreads = cpus -1  # Use this number of threads for training\n\n\n\n%%time\n\nw2v_model = Word2Vec(\n    sentences=tokenized_data, vector_size=dim_w2v, alpha=alpha, min_alpha=alpha_min, window=wnd, \n    min_count=mincount, sample=sample, sg=sg, negative=ngt, epochs=epochs, workers=threads\n)\n\nCPU times: user 1min 31s, sys: 1.19 s, total: 1min 32s\nWall time: 53.7 s\n\n\nLet’s double-check the number of words present in each of our two models:\n\n\nCode\nprint('Size of LSI vocab.:', len(dct.keys()))\nprint('Size of w2v vocab.:', len(w2v_model.wv.key_to_index.keys()))\n\n\nSize of LSI vocab.: 42439\nSize of w2v vocab.: 42439\n\n\nWe’ve hence managed to build an LSI and a word2vec model whose vocabularies contain the exact same words—great! However, this came at an unnecessarily high price and we’ll shortly see why. What happens behind the scenes when we create a new instance of the Word2Vec class is the following. First a quick sanity check of the corpus is performed, then the vocabulary is built using the .build_vocab() method, and lastly the method .train() is executed, which trains the model. In the second step, a new dictionary is built from scratch, despite having already done so for the LSI model. When working with small datasets this procedure might be acceptable, but when the corpus is very large optimising this step can save a lot of time!"
  },
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary/index.html#lsi-model-1",
    "href": "posts/2021-07-27-gensim-shared-vocabulary/index.html#lsi-model-1",
    "title": "Gensim: share vocabulary across models",
    "section": "LSI model",
    "text": "LSI model\nAs we’ve seen before, the first step is to create a dictionary. Before we passed a list to Dictionary, now we pass it a generator:\n\ncurpus_path = Path('20news.txt')\ndct = Dictionary((tokenizer(line) for line in open(curpus_path)))\n\nStep two consists in creating a corpus and switching to a TF-IDF representation. Here is where things change a bit. We need to define an iterable that yields documents in BoW representation, which is done by the Corpus class here below.\n\nclass Corpus:\n    '''Iterable that yields BoW representations of documents.'''\n    \n    def __init__(self, curpus_path, dct_object):\n        self.curpus_path = curpus_path\n        self.dct_object = dct_object\n        \n    def __iter__(self):\n        for line in sopen(self.curpus_path):\n            yield self.dct_object.doc2bow(tokenizer(line))\n\nWe then use it to create our streamed corpus, which can be passed to TfidfModel. We’ll skip the explicit creation of the TF-IDF matrix because it can be very large.\n\ncorpus = Corpus(curpus_path, dct)\ntfidf_model = TfidfModel(corpus, id2word=dct)\n\nWe are now ready to build our LSI model:\n\n%%time\n\nlsi_model = LsiModel(corpus=tfidf_model[corpus], id2word=dct, num_topics=dim_lsi)\n\nCPU times: user 3min 58s, sys: 11 s, total: 4min 9s\nWall time: 3min 17s\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe need to use iterables and not generators even though they both produce an iterator. This is because after we have exhausted a generator once there is no more data available. In contrast, iterables create a new iterator every time they are looped over. This is exactly what we need when creating a model: we need to be able to iterate over a dataset more than once."
  },
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary/index.html#word2vec-model-1",
    "href": "posts/2021-07-27-gensim-shared-vocabulary/index.html#word2vec-model-1",
    "title": "Gensim: share vocabulary across models",
    "section": "word2vec model",
    "text": "word2vec model\nSimilar to what we did with the LSI model, we need to define an iterable that yields tokenized documents. This is provided by the CorpusW2V class below.\n\nclass CorpusW2V:\n    '''Iterable that yields sentences (lists of str).'''\n\n    def __init__(self, curpus_path):\n        self.curpus_path = curpus_path\n\n    def __iter__(self):\n        for line in sopen(self.curpus_path):\n            yield tokenizer(line)\n\ncorpus_w2v = CorpusW2V(curpus_path)\n\nThe rest follows exactly as above, with the only difference that now the .train() method receives an instance of the CorpusW2V class instead of a list (see tokenized_data above).\n\n%%time\n\nw2v_model = Word2Vec(\n    vector_size=dim_w2v, alpha=alpha, min_alpha=alpha_min, window=wnd, \n    min_count=mincount, sample=sample, sg=sg, negative=ngt, workers=threads\n)\n# Borrow LSI vocab.\nword_freq = {dct[k]: v for k,v in dct.cfs.items()}\nw2v_model.build_vocab_from_freq(word_freq)\n# Train model\nnum_samples = dct.num_docs\nw2v_model.train(corpus_w2v, total_examples=num_samples, epochs=epochs)\n\nCPU times: user 5min 52s, sys: 4.78 s, total: 5min 56s\nWall time: 6min 31s\n\n\n(2608743, 6277620)\n\n\nWe conclude by noting that this approach based on data streaming is certainly slower than when we load everything into memory. However, it allows us to process arbitrarily large datasets. One can’t have it all, as they say."
  },
  {
    "objectID": "posts/2022-02-06-unknown-label-problem/index.html",
    "href": "posts/2022-02-06-unknown-label-problem/index.html",
    "title": "The unknown label problem",
    "section": "",
    "text": "We often train machine learning models to assign samples to a class. Take for instance the MNIST dataset, where images of handwritten digits between 0 and 9 have to be assigned to one of 10 possible classes. Another example, and this is the dataset we’ll consider in this post, is the Oxford-IIIT Pet Dataset, which contains more than 7000 images of cats and dogs from 37 different breeds. Our goal is to assign the right breed to each image. The scenario just described is typically approached as a multi-class classification problem, in which samples have to be assigned to one of \\(N\\) possible classes.\nLet’s say we have already trained our pet breed classifier and want to roll it out so that other people can interact with it. In such an environment no one can prevent a user from loading an image that does not belong to any of the classes known to our model. And even if someone were to do so, our model would still return one of the 37 breeds—it has no ability to say “it’s none of the breeds I know”. Our goal is to create a model that can do just that!\nAlthough this seems to be a fairly common problem, I have seen many people struggling to find a solution. One might be tempted to add an additional class (let’s call it is_other) to which all samples that do not contain any instance of a known class will be assigned. However, as Jeremy Howard explains in one of the lectures of his course Deep Learning from the Foundations, this is not a great idea. The main reason is that our model would have to learn the features that distinguish the negative of every other class we are interested in. An approach that might work better instead is to treat our problem as if it were a multi-label classification task.\n\n\n\n\n\n\nNote\n\n\n\nYou can reproduce all the results presented below by accessing the notebook used to create this post. If you do so, make sure you have access to a GPU. This speeds up the execution time considerably!"
  },
  {
    "objectID": "posts/2022-02-06-unknown-label-problem/index.html#constructing-a-datablock-and-dataloaders",
    "href": "posts/2022-02-06-unknown-label-problem/index.html#constructing-a-datablock-and-dataloaders",
    "title": "The unknown label problem",
    "section": "Constructing a DataBlock and DataLoaders",
    "text": "Constructing a DataBlock and DataLoaders\nThose already familiar with PyTorch know that there are two main classes for accessing a dataset: the Dataset and DataLoader. On top of that, fastai offers two classes, the Datasets and DataLoaders, for bringing traning and validation Datasets and DataLoaders together.\nWe’ll now create a DataBlock, which is sort of template for creating a DataLoaders later on. Here is the code, with an explanation of each line immediately afterwards.\n\nname_regex = r\"(.+)_\\d+\\.jpg$\"\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(seed=42),\n    get_y=using_attr(RegexLabeller(name_regex), attr='name'),\n    item_tfms=Resize(460),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75)\n)\n\nFirst, we need to specify what data we are working with. Our independent variables are images (hence ImageBlock), while our dependent variables are labels (hence CategoryBlock, which returns a single integer). Second, we need to tell fastai how to get the list of items in our dataset. As we saw before, the function get_image_files does exactly that. We then decide to split our data randomly, which is what the RandomSplitter does. By default, it keeps 80% of the data for training and puts the remaining 20% in the validation set. The DataBlock now knows how go get and split data, but it doesn’t know how to label these items. We specify this with the get_y argument. A handy class to label samples with regular expressions is RegexLabeller. Note that we cannot pass items directly to it as they are—the elements in fns are pathlib.Path objects, not strings (see above)—but we have to use the .name attribute. This is what using_attr does. Lastly, we resize every image to the same size (460 x 460 pixels) so that we can apply data augmentation to batches on the GPU.\nWe can now build our DataLoaders! But before moving on, we want to make sure that things have gone as planned. To do so, we’ll check out a few samples.\n\n# Create DataLoaders\ndls = dblock.dataloaders(path/\"images\")\ndls.show_batch(nrows=1, ncols=5)"
  },
  {
    "objectID": "posts/2022-02-06-unknown-label-problem/index.html#model-training",
    "href": "posts/2022-02-06-unknown-label-problem/index.html#model-training",
    "title": "The unknown label problem",
    "section": "Model training",
    "text": "Model training\nOnce we are convinced that our data have been prepared correctly, we can proceed with the creation of a deep learning model. We’re going to keep things simple here as we want the training to be short. For this reason, we’ll proceed with a ResNet18 model, which is a pretty small Convolutional Neural Network (CNN) by today’s standards.\nThe learning rate is one of the hyperparameters that has the greatest influence on the training of neural networks. Practitioners usually set it to a value they like or tune it via hyperparameter sweeps. fastai provides us with the lr_find method, which implements a version of the learning rate finder originally proposed by Leslie Smith.\n\n# Create CNN model (ResNet18)\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy])\nlearn.lr_find(start_lr=1e-6)\n\n\n\n\nSuggestedLRs(valley=0.0014125375309959054)\n\n\n\n\n\nWe opt for an aggressive value for the learning rate, which allows us to train the model for a small number of epochs. We’ll train the model’s head for 1 epoch, while keeping the rest of the weights frozen; then we unfreeze all the weights and continue the training for 4 more epochs.\nAll this is packaged in the fine_tune method.\n\n# Fine-tune model\nlearn.fine_tune(4, base_lr=3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.403973\n      0.382155\n      0.121110\n      0.878890\n      01:25\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.518523\n      0.352880\n      0.117050\n      0.882950\n      01:29\n    \n    \n      1\n      0.456349\n      0.321530\n      0.097429\n      0.902571\n      01:34\n    \n    \n      2\n      0.308262\n      0.233842\n      0.072395\n      0.927605\n      01:29\n    \n    \n      3\n      0.176608\n      0.223449\n      0.065629\n      0.934371\n      01:29\n    \n  \n\n\n\nWe can already see from this table that the training was quite successful—the error rate is less than 7%. We can now take a look at some predictions on the validation set, which can be obtained through the method show_results.\n\n# Display couple of results\nlearn.show_results(max_n=5, nrows=1)\n\n\n\n\n\n\n\nNow that we are confident enough that our model is working well, we can put it to the test by passing it some pictures found on the web. Let’s start with the image of a cat whose breed is known to our model and see how it behaves.\n\nimg_cat = PILImage.create('maine_coon.jpeg')\nimg_cat.show();\n\n\n\n\n\nlearn.predict(img_cat)[0]\n\n\n\n\n'Maine_Coon'\n\n\nThe model correctly predicts that it is a maine coon cat—so far so good! Let’s try now with a picture of something it has never seen before, for example an elephant?\n\nimg_el = PILImage.create('elephant.jpg')\nimg_el.show();\n\n\n\n\n\nlearn.predict(img_el)[0]\n\n\n\n\n'British_Shorthair'\n\n\nHere the model tells us that it is a british shorthair cat. Instead, we would like the model to tell us when it doesn’t know something. However, the current model has no way of doing this because it was built that way. Note that when we’ve instantiated our Learner above, we didn’t specify what loss function to use. This is because fastai is smart enough to select the right one for us, which in this case is the cross-entropy loss. Also, to turn the activations into actual predictions, the last layer uses the softmax activation function. By construction, the softmax really wants to pick one class among the available—after all, the outputs of a softmax layer have to add up to 1 and the largest activation value is magnified even more. In the next section, we will see how by reformulating the problem we can achieve the desired behavior.\n\n\n\n\n\n\nTip\n\n\n\nIf you have not set the loss function yourself and you are unsure of what fastai has chosen for you, you can always inspect the .loss_func attribute of your Learner."
  },
  {
    "objectID": "posts/2022-02-06-unknown-label-problem/index.html#constructing-a-datablock-and-dataloaders-1",
    "href": "posts/2022-02-06-unknown-label-problem/index.html#constructing-a-datablock-and-dataloaders-1",
    "title": "The unknown label problem",
    "section": "Constructing a DataBlock and DataLoaders",
    "text": "Constructing a DataBlock and DataLoaders\nIn the multi-class case, we used the CategoryBlock, while here we’ll switch to the MultiCategoryBlock, which returns a one-hot encoded vector. This block expects a list of labels (strings), hence we need to modify a bit the get_y function, which was only returning the pet breed as a string. The next cell defines this function, builds new DataBlock and DataLoaders, and displays a few samples.\n\ndef get_y(fp):\n    \"\"\"Get list of labels from df.\"\"\"\n    lbl = using_attr(RegexLabeller(name_regex), attr=\"name\")(fp)\n    return [lbl]\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(seed=42),\n    get_y=get_y,\n    item_tfms=Resize(460),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75),\n)\n\ndls = dblock.dataloaders(path/\"images\")\ndls.show_batch(nrows=1, ncols=5)"
  },
  {
    "objectID": "posts/2022-02-06-unknown-label-problem/index.html#model-training-1",
    "href": "posts/2022-02-06-unknown-label-problem/index.html#model-training-1",
    "title": "The unknown label problem",
    "section": "Model training",
    "text": "Model training\nBefore creating a new model and launching the learning rate finder, we’d like to point out some differences in relation to what we have done above. First, even if we don’t do so explicitly, fastai will again choose the correct loss function for us, which in this case is the binary cross-entropy loss. The activation function of the final layer is no longer the softmax, rather the sigmoid function. The outputs of this layer therefore no longer have to add up to one, but each neuron can return a value between 0 and 1 regardless of the others. This means that we also need to adapt our metric. In the multi-class case, the accuracy was comparing the target label with the class with highest activation (via argmax). But now these activations might even be all 1 in theory, or all 0. We hence need to pick a threshold to decide what has to be set to 0 and what to 1. This is what accuracy_multi does.\n\n# Create CNN model (ResNet18)\nlearn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=0.7))\nlearn.lr_find(start_lr=1e-6)\n\n\n\n\nSuggestedLRs(valley=0.003162277629598975)\n\n\n\n\n\nWe choose again a learning rate value large enough to have a reasonably fast training and launch it.\n\n# Fine-tune model\nlearn.fine_tune(4, base_lr=1e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.319909\n      0.029859\n      0.981696\n      01:30\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.034611\n      0.028626\n      0.986578\n      01:33\n    \n    \n      1\n      0.030179\n      0.023405\n      0.988754\n      01:33\n    \n    \n      2\n      0.018344\n      0.015487\n      0.992576\n      01:32\n    \n    \n      3\n      0.010457\n      0.013025\n      0.993472\n      01:32\n    \n  \n\n\n\nThe results seem promising, so we move on and display a few predictions on the validation set.\n\n# Display couple of results\nlearn.show_results(max_n=5, nrows=1)\n\n\n\n\n\n\n\nThe crucial moment is approaching: we want to see how the model behaves when we show it new pictures and, in particular, pictures depicting things it doesn’t know. We hence repeat the previous experiment by passing it the image of a cat.\n\n# Increase threshold, i.e. assign a label only when very confident.\nlearn.loss_func = BCEWithLogitsLossFlat(thresh=0.7)\n\nlearn.predict(img_cat)[0]\n\n\n\n\n(#1) ['Maine_Coon']\n\n\nAll good for now, it correctly predicts that it’s a maine coon. But what about the image of the elephant?\n\nlearn.predict(img_el)[0]\n\n\n\n\n(#0) []\n\n\nGreat! Our model doesn’t recognise the content of this picture and hence doesn’t return any label! 🎉"
  },
  {
    "objectID": "posts/2023-06-29-entity-embeddings/index.html",
    "href": "posts/2023-06-29-entity-embeddings/index.html",
    "title": "Entity embeddings of categorical variables",
    "section": "",
    "text": "Despite the buzz around generative AI, most applications in industry still originate from tabular datasets. In tabular data, some columns may represent numerical variables, like atmospheric pressure, while others may be categorical variables, like sex or product categories. These can take only a limited number of values. The path to using numerical variables is relatively smooth and requires little preprocessing (for some algorithms, even none at all). In contrast, categorical variables must first be converted into numbers, as this is what a computer can process, and this can be done in many ways.\nTwo conventional approaches to encode categorical variables are ordinal encoding and one-hot encoding. Ordinal encoding assigns each unique value to a different integer and, as such, assumes an ordering of the categories. For instance, “Size” could be the name of a categorical columns with values: small < medium < large, which would be mapped to 0, 1, and 2, respectively. One-hot encoding creates instead a new column for each possible value in the original data indicating its presence or absence. If the first sample in the dataset were large, we would get the following one-hot representation: (0, 0, 1). Unlike ordinal encoding, one-hot encoding does not assume an ordering of the categories.\nHowever, both ordinal and one-hot encodings have their limitations. To name a few, in ordinal encoding, the assigned numerical values may introduce unintended relationships or orders between categories that do not exist in the original data. One-hot encoding, while solving the issue of introducing unintended order, can lead to high-dimensional and sparse representations for features with a large number of unique categories. This translates into increased computational complexity and memory usage.\nIn 2015, the Rossmann sales competition took place on Kaggle. The solution of one of the gold medal winners clearly diverged from the others by using a deep learning model, in one of the first known examples of a cutting-edge deep learning model for tabular data. Rather than using traditional encoding methods, the authors introduced the concept of Entity Embeddings. Entity Embeddings provide a way to represent categorical variables as low-dimensional continuous vectors, capturing the underlying semantic relationships between categories. This approach eliminates the limitations of ordinal encoding’s introduced order and one-hot encoding’s curse of dimensionality. Their approach is summarised in the paper Entity Embeddings of Categorical Variables, by Cheng Guo and Felix Berkhahn.\nWe’ll explore here the concepts and advantages of Entity Embeddings, diving deep and trying to replicate the main findings of the paper. Along the way, we’ll offer a comprehensive understanding of their applications in general machine learning models."
  },
  {
    "objectID": "posts/2023-06-29-entity-embeddings/index.html#trainvalid-split",
    "href": "posts/2023-06-29-entity-embeddings/index.html#trainvalid-split",
    "title": "Entity embeddings of categorical variables",
    "section": "Train/Valid split",
    "text": "Train/Valid split\nOur training data spans approximately 2.5 years from 2013 to mid-2015. The test data covers instead the subsequent part of 2015, with no overlap with the training set dates. To test the generalisation capability of our model, we’ll try to put ourselves in the closest possible situation to this by sorting the training data by Date and keeping the last 10% of the samples for validation. In this way, we use the older samples for training and the most recent ones for validation. This should ensure that the performance observed on the validation set is as close as possible to that of the leaderboard after submission.\nWe’ll also drop samples from the validation set for which there are zero sales. This is to align ourselves with the competition, whose website states that “Any day and store with 0 sales is ignored in scoring.”.1\n\n\nCode\ntgt = 'Sales'  # Target variable\ntrain_df = train_df.sort_values(by=['Date']).reset_index(drop=True)\n\ntrain_xs, valid_xs, train_y, valid_y = train_test_split(\n    train_df.drop(columns=tgt), train_df[tgt], test_size=0.1, shuffle=False\n)\n\nvalid_xs, valid_y = drop_zero_sales(valid_xs, valid_y)\n\nprint(f'Train size: {len(train_y):>6}')\nprint(f'Valid size: {len(valid_y):>6}')\n\n\nTrain size: 759952\nValid size:  84439"
  },
  {
    "objectID": "posts/2023-06-29-entity-embeddings/index.html#ordinal-encoding",
    "href": "posts/2023-06-29-entity-embeddings/index.html#ordinal-encoding",
    "title": "Entity embeddings of categorical variables",
    "section": "Ordinal encoding",
    "text": "Ordinal encoding\nAs a first (and only) operation to prepare the data, we enrich the representation of dates. We keep it simple and only create new columns for Day, Month, and Year out of Date. After all, there are already the DayOfWeek, StateHoliday, and SchoolHoliday columns providing additional information in this regard.\n\n\nCode\ndef proc_data(df: pd.DataFrame):\n    \"Process DataFrame and create date features inplace.\"\n    df['Day'] = df.Date.dt.day\n    df['Month'] = df.Date.dt.month\n    df['Year'] = df.Date.dt.year\n    df['Open'] = df.Open.fillna(1).astype(int)\n\nproc_data(train_xs)\nproc_data(valid_xs)\nproc_data(test_df)\n\n\nWe’re now ready to map our categorical variables to integers using scikit-learn’s OrdinalEncoder. In addition, we only select the relevant features.\n\ncats = ['Store', 'DayOfWeek', 'Day', 'Month', 'Year', 'Promo', 'StateHoliday', 'SchoolHoliday']\n\noe = OrdinalEncoder(dtype=int)\ntrain_xs[cats] = oe.fit_transform(train_xs[cats])\nvalid_xs[cats] = oe.transform(valid_xs[cats])\ntest_df[cats] = oe.transform(test_df[cats])\n\ntrain_xs, valid_xs, test_xs = train_xs[cats], valid_xs[cats], test_df[cats]\n\nLet’s have a look at our preprocessed dataframe, which is now ready to be passed to our first model.\n\ntrain_xs.tail(3)\n\n\n\n\n\n  \n    \n      \n      Store\n      DayOfWeek\n      Day\n      Month\n      Year\n      Promo\n      StateHoliday\n      SchoolHoliday\n    \n  \n  \n    \n      759949\n      351\n      5\n      1\n      4\n      2\n      0\n      0\n      0\n    \n    \n      759950\n      350\n      5\n      1\n      4\n      2\n      0\n      0\n      0\n    \n    \n      759951\n      364\n      5\n      1\n      4\n      2\n      0\n      0\n      0"
  },
  {
    "objectID": "posts/2023-06-29-entity-embeddings/index.html#training-the-embeddings",
    "href": "posts/2023-06-29-entity-embeddings/index.html#training-the-embeddings",
    "title": "Entity embeddings of categorical variables",
    "section": "Training the embeddings",
    "text": "Training the embeddings\nThe deep learning framework we will use to build and train our neural network is PyTorch. In PyTorch, a Dataset is constructed by subclassing Dataset and requires us to override two dunder methods: __getitem__ and __len__. In the following we implement our Dataset to return a tensor for the features and one for the target for the training and validation set, whereas only the features for the test set.\n\n\nCode\nclass TabDataset(Dataset):\n    \"Tabular Dataset that yields categorical features and target.\"\n\n    def __init__(self, feats, new_data=True, tgt=None):\n        self.x_cat = torch.tensor(feats.values, dtype=torch.int32)\n        self.n_samples = len(feats)\n        self.new_data = new_data\n        \n        if not new_data:\n            self.y = torch.tensor(tgt.values, dtype=torch.float32).view(-1, 1)\n\n    def __len__(self):\n        return self.n_samples\n\n    def __getitem__(self, idx: int):\n        if not self.new_data:\n            return self.x_cat[idx], self.y[idx]\n        return self.x_cat[idx]\n\n\n\ntrain_ds = TabDataset(train_xs, new_data=False, tgt=train_y)\nvalid_ds = TabDataset(valid_xs, new_data=False, tgt=valid_y)\n\nNext, we can build the training and validation DataLoaders, which take a Dataset and return an iterable that handles shuffling, batching, and all the rest for us.\n\nbs = 1024  # Batch size\ntrain_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=bs*2, shuffle=False)\n\nNow comes the main part, the construction of the neural network that will learn the entity embeddings. In PyTorch, we can easily create custom models by subclassing the nn.Module module. In a nutshell, this neural network consists of an initial part of embedding layers—these represent our matrices of latent factors—followed by a fully connected part.\n\nclass TabularNeuralNet(nn.Module):\n    \"Neural network model for tabular data.\"\n\n    def __init__(\n        self,\n        emb_szs: list,  # List of (unique_cats, embedding_dim)\n        out_sz: int,    # Number of outputs for final layer\n        layers: list,   # Size of the hidden layers\n        act_cls=nn.ReLU(inplace=True),  # Activation type after `Linear` layers\n    ):\n        super().__init__()\n        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_szs])\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb = n_emb\n        sizes = [n_emb] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes) - 2)] + [None]\n        _layers = [LinReLu(sizes[i], sizes[i + 1], act=a) for i, a in enumerate(actns)]\n        self.layers = nn.Sequential(*_layers)\n\n    def forward(self, x_cat):\n        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeds)]\n        x = torch.cat(x, 1)\n        return self.layers(x)\n\nThe only thing that remains for us to do is to determine the emb_szs parameter. The part concerning the number of unique categories can easily be found using the OrdinalEncoder we fitted above, but the embedding size for each categorical feature is a free parameter. As such, there is no clear-cut way to determine it but it’s typically much smaller than the number of unique categories. We are thus thankful that the authors report these values in their paper.\n\n\nCode\nnum_unique = [len(c) for c in oe.categories_]\nemb_dims = [10, 6, 10, 6, 2, 1, 3, 1]\nembed_sizes = [(u, e) for u, e in zip(num_unique, emb_dims)]\n\n\n\nnn_model = TabularNeuralNet(emb_szs=embed_sizes, out_sz=1, layers=[200, 100])\n\nWhat follows is the classic PyTorch training loop, which we encapsulate for convenience in two functions, train_one_epoch and validate_one_epoch, that do what they say: train and validate the model over one epoch, respectively. We train our model for a few epochs using the standard Mean Squared Error (MSE) loss function and Adam optimiser.\n\nfor ep in range(EPOCHS):\n    print(f\"Epoch {ep+1}\\n\" + \"-\"*31)\n    train_one_epoch(train_dl, nn_model, loss_func, optim)\n    validate_one_epoch(valid_dl, nn_model, loss_func)\n\nEpoch 1\n-------------------------------\nLoss: 56439536  [     0/759952]\nLoss:  1829678  [409600/759952]\nLoss:  2018285  [759952/759952]\nAvg. valid. loss: 1823780, Avg. RMSPE: 0.2195\n\nEpoch 2\n-------------------------------\nLoss:  1885142  [     0/759952]\nLoss:  1538159  [409600/759952]\nLoss:  1631149  [759952/759952]\nAvg. valid. loss: 1681688, Avg. RMSPE: 0.2059\n\nEpoch 3\n-------------------------------\nLoss:  1414050  [     0/759952]\nLoss:  1237316  [409600/759952]\nLoss:   899826  [759952/759952]\nAvg. valid. loss: 1374336, Avg. RMSPE: 0.1720\n\n\n\nThat’s it! By training our neural network we obtained the entity embeddings. As a nice side-product we also got a fully functioning model that we can use to forecast the sales.\n\n\nCode\nprint(f\"Valid RMSPE: {rmspe(all_preds, valid_y.values.reshape(-1, 1)):.3f}\")\nprint(f\"Valid MAPE:  {mape(all_preds, valid_y.values.reshape(-1, 1)):.3f}\")\n\n\nValid RMSPE: 0.179\nValid MAPE:  0.123\n\n\nIt turns out that this model works much better than our previous baseline, lowering both RMSPE and MAPE substantially. Submitting these results would bring the score on the private test set down to about 0.17. We are definitely moving in the right direction!"
  },
  {
    "objectID": "posts/2023-06-29-entity-embeddings/index.html#random-forests-with-entity-embeddings",
    "href": "posts/2023-06-29-entity-embeddings/index.html#random-forests-with-entity-embeddings",
    "title": "Entity embeddings of categorical variables",
    "section": "Random Forests with Entity Embeddings",
    "text": "Random Forests with Entity Embeddings\nNow that we have trained the neural network, we have the entity embeddings at our disposal. All that remains is to extract them from the embedding layer of our TabularNeuralNet. We write a function, embed_features, to do that since we’ll have to repeat this operation for the training, validation and test set2.\n\n\nCode\ndef embed_features(model, xs, encoder):\n    \"Replace categorical columns in `xs` w/ embeddings extracted from `model`.\"\n    xs = xs.copy()\n\n    with torch.no_grad():\n        for i, col in enumerate(encoder.feature_names_in_):\n            # Get embedding matrix\n            emb = model.embeds[i]\n            emb_data = emb(torch.tensor(xs[col].values, dtype=torch.int32))\n            emb_names = [f'{col}_{j}' for j in range(emb_data.shape[1])]\n\n            # Replace old feature col. w/ new one(s)\n            feat_df = pd.DataFrame(\n                data=emb_data.cpu().numpy(), index=xs.index, columns=emb_names\n            )\n            xs = xs.drop(col, axis=1).join(feat_df)\n    return xs\n\n\nWe are now ready to apply the mapping defined by the embeddings to our original features. We also show the first few rows to see what the result looks like.\n\n# Train embeddings\nemb_train_xs = embed_features(nn_model, train_xs, oe)\n\n# Validation embeddings\nemb_valid_xs = embed_features(nn_model, valid_xs, oe)\n\n\nemb_train_xs.head(3)\n\n\n\n\n\n  \n    \n      \n      Store_0\n      Store_1\n      Store_2\n      Store_3\n      Store_4\n      Store_5\n      Store_6\n      Store_7\n      Store_8\n      Store_9\n      ...\n      Month_3\n      Month_4\n      Month_5\n      Year_0\n      Year_1\n      Promo_0\n      StateHoliday_0\n      StateHoliday_1\n      StateHoliday_2\n      SchoolHoliday_0\n    \n  \n  \n    \n      0\n      -0.464628\n      1.389546\n      0.031453\n      0.837222\n      -2.890536\n      -1.271165\n      0.351642\n      0.352165\n      -1.067315\n      0.432503\n      ...\n      0.912523\n      1.552763\n      -0.231177\n      -0.952061\n      0.123717\n      0.058928\n      0.693458\n      -1.862958\n      -0.175023\n      -1.131267\n    \n    \n      1\n      0.548922\n      0.104170\n      0.247237\n      0.384087\n      -1.878805\n      -1.581414\n      1.528581\n      -0.838187\n      -1.658705\n      -2.186104\n      ...\n      0.912523\n      1.552763\n      -0.231177\n      -0.952061\n      0.123717\n      0.058928\n      0.693458\n      -1.862958\n      -0.175023\n      -1.131267\n    \n    \n      2\n      -0.613926\n      -0.435071\n      -0.327684\n      2.248206\n      -0.140331\n      -0.281201\n      0.297878\n      2.848119\n      1.672459\n      1.167650\n      ...\n      0.912523\n      1.552763\n      -0.231177\n      -0.952061\n      0.123717\n      0.058928\n      0.693458\n      -1.862958\n      -0.175023\n      -1.131267\n    \n  \n\n3 rows × 39 columns\n\n\n\nWe can see that our initial dataset, which had 8 columns representing categorical variables, now reaches 39 columns. This number amounts to the sum of the embedding sizes we have chosen.\nWe can now retrain our initial model, i.e. the Random Forests, using these new features created by the neural network.\n\nemb_m = rf(emb_train_xs.values, train_y.values, max_features=0.6)\n\n\n\n\n\n\n\nNote\n\n\n\nSince we now have more columns, the model will take longer to train. We therefore set the parameter max_features to 0.6, which defines how many columns to sample at each split point (i.e. 60% of the total).\n\n\nFinally, the moment we have all been anxiously waiting for has arrived. Will our model have benefited from the use of Entity Embeddings? 🥁\n\n\nCode\nemb_valid_xs, valid_y_clean = drop_zero_sales(emb_valid_xs, valid_y)\n\nprint(f\"Valid RMSPE: {m_rmspe(emb_m, emb_valid_xs.values, valid_y_clean.values):.3f}\")\nprint(f\"Valid MAPE:  {m_mape(emb_m, emb_valid_xs.values, valid_y_clean.values):.3f}\")\n\n\nValid RMSPE: 0.139\nValid MAPE:  0.101\n\n\nThat’s definitely the case! We started from an RMSPE of about 0.31 using ordinal encoding to arrive at 0.14 with Entity Embeddings—we’ve halved the error! The score on the private test set confirms the results and the positive trend, bringing the RSMPE down to 0.142."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "📮 Posts",
    "section": "",
    "text": "Entity embeddings of categorical variables\n\n\n\n\n\n\n\nregression\n\n\nrandom forests\n\n\nembeddings\n\n\npytorch\n\n\n\n\nBringing the power of neural nets to tree-based models.\n\n\n\n\n\n\nSep 19, 2023\n\n\nLuca Papariello\n\n\n\n\n\n\n  \n\n\n\n\nThe unknown label problem\n\n\n\n\n\n\n\nfastai\n\n\ncomputer vision\n\n\nmulti-class classification\n\n\nmulti-label classification\n\n\n\n\nWhy turning a multi-class classification task into a multi-label one might be a good idea.\n\n\n\n\n\n\nFeb 6, 2022\n\n\nLuca Papariello\n\n\n\n\n\n\n  \n\n\n\n\nGensim: share vocabulary across models\n\n\n\n\n\n\n\nnlp\n\n\ngensim\n\n\n\n\nA brief illustration of how to share a common vocabulary among different Gensim models.\n\n\n\n\n\n\nJul 27, 2021\n\n\nLuca Papariello\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "After repeatedly finding help in a very welcoming community, I thought it was time to give back some of that warmth through this blog. It can be seen as a collection of personal notes and/or tricks discovered while working on my projects."
  }
]