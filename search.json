[
  {
    "objectID": "posts/2022-02-06-unknown-label-problem.html",
    "href": "posts/2022-02-06-unknown-label-problem.html",
    "title": "The unknown label problem",
    "section": "",
    "text": "We often train machine learning models to assign samples to a class. Take for instance the MNIST dataset, where images of handwritten digits between 0 and 9 have to be assigned to one of 10 possible classes. Another example, and this is the dataset weâ€™ll consider in this post, is the Oxford-IIIT Pet Dataset, which contains more than 7000 images of cats and dogs from 37 different breeds. Our goal is to assign the right breed to each image. The scenario just described is typically approached as a multi-class classification problem, in which samples have to be assigned to one of \\(N\\) possible classes.\nLetâ€™s say we have already trained our pet breed classifier and want to roll it out so that other people can interact with it. In such an environment no one can prevent a user from loading an image that does not belong to any of the classes known to our model. And even if someone were to do so, our model would still return one of the 37 breedsâ€”it has no ability to say â€œitâ€™s none of the breeds I knowâ€. Our goal is to create a model that can do just that!\nAlthough this seems to be a fairly common problem, I have seen many people struggling to find a solution. One might be tempted to add an additional class (letâ€™s call it is_other) to which all samples that do not contain any instance of a known class will be assigned. However, as Jeremy Howard explains in one of the lectures of his course Deep Learning from the Foundations, this is not a great idea. The main reason is that our model would have to learn the features that distinguish the negative of every other class we are interested in. An approach that might work better instead is to treat our problem as if it were a multi-label classification task.\n\n\n\n\n\n\nNote\n\n\n\nYou can reproduce all the results presented below by accessing the notebook used to create this post. If you do so, make sure you have access to a GPU. This speeds up the execution time considerably!"
  },
  {
    "objectID": "posts/2022-02-06-unknown-label-problem.html#constructing-a-datablock-and-dataloaders",
    "href": "posts/2022-02-06-unknown-label-problem.html#constructing-a-datablock-and-dataloaders",
    "title": "The unknown label problem",
    "section": "Constructing a DataBlock and DataLoaders",
    "text": "Constructing a DataBlock and DataLoaders\nThose already familiar with PyTorch know that there are two main classes for accessing a dataset: the Dataset and DataLoader. On top of that, fastai offers two classes, the Datasets and DataLoaders, for bringing traning and validation Datasets and DataLoaders together.\nWeâ€™ll now create a DataBlock, which is sort of template for creating a DataLoaders later on. Here is the code, with an explanation of each line immediately afterwards.\n\nname_regex = r\"(.+)_\\d+\\.jpg$\"\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(seed=42),\n    get_y=using_attr(RegexLabeller(name_regex), attr='name'),\n    item_tfms=Resize(460),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75)\n)\n\nFirst, we need to specify what data we are working with. Our independent variables are images (hence ImageBlock), while our dependent variables are labels (hence CategoryBlock, which returns a single integer). Second, we need to tell fastai how to get the list of items in our dataset. As we saw before, the function get_image_files does exactly that. We then decide to split our data randomly, which is what the RandomSplitter does. By default, it keeps 80% of the data for training and puts the remaining 20% in the validation set. The DataBlock now knows how go get and split data, but it doesnâ€™t know how to label these items. We specify this with the get_y argument. A handy class to label samples with regular expressions is RegexLabeller. Note that we cannot pass items directly to it as they areâ€”the elements in fns are pathlib.Path objects, not strings (see above)â€”but we have to use the .name attribute. This is what using_attr does. Lastly, we resize every image to the same size (460 x 460 pixels) so that we can apply data augmentation to batches on the GPU.\nWe can now build our DataLoaders! But before moving on, we want to make sure that things have gone as planned. To do so, weâ€™ll check out a few samples.\n\n# Create DataLoaders\ndls = dblock.dataloaders(path/\"images\")\ndls.show_batch(nrows=1, ncols=5)"
  },
  {
    "objectID": "posts/2022-02-06-unknown-label-problem.html#model-training",
    "href": "posts/2022-02-06-unknown-label-problem.html#model-training",
    "title": "The unknown label problem",
    "section": "Model training",
    "text": "Model training\nOnce we are convinced that our data have been prepared correctly, we can proceed with the creation of a deep learning model. Weâ€™re going to keep things simple here as we want the training to be short. For this reason, weâ€™ll proceed with a ResNet18 model, which is a pretty small Convolutional Neural Network (CNN) by todayâ€™s standards.\nThe learning rate is one of the hyperparameters that has the greatest influence on the training of neural networks. Practitioners usually set it to a value they like or tune it via hyperparameter sweeps. fastai provides us with the lr_find method, which implements a version of the learning rate finder originally proposed by Leslie Smith.\n\n# Create CNN model (ResNet18)\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy])\nlearn.lr_find(start_lr=1e-6)\n\n\n\n\nSuggestedLRs(valley=0.0014125375309959054)\n\n\n\n\n\nWe opt for an aggressive value for the learning rate, which allows us to train the model for a small number of epochs. Weâ€™ll train the modelâ€™s head for 1 epoch, while keeping the rest of the weights frozen; then we unfreeze all the weights and continue the training for 4 more epochs.\nAll this is packaged in the fine_tune method.\n\n# Fine-tune model\nlearn.fine_tune(4, base_lr=3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.403973\n      0.382155\n      0.121110\n      0.878890\n      01:25\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.518523\n      0.352880\n      0.117050\n      0.882950\n      01:29\n    \n    \n      1\n      0.456349\n      0.321530\n      0.097429\n      0.902571\n      01:34\n    \n    \n      2\n      0.308262\n      0.233842\n      0.072395\n      0.927605\n      01:29\n    \n    \n      3\n      0.176608\n      0.223449\n      0.065629\n      0.934371\n      01:29\n    \n  \n\n\n\nWe can already see from this table that the training was quite successfulâ€”the error rate is less than 7%. We can now take a look at some predictions on the validation set, which can be obtained through the method show_results.\n\n# Display couple of results\nlearn.show_results(max_n=5, nrows=1)\n\n\n\n\n\n\n\nNow that we are confident enough that our model is working well, we can put it to the test by passing it some pictures found on the web. Letâ€™s start with the image of a cat whose breed is known to our model and see how it behaves.\n\nimg_cat = PILImage.create('maine_coon.jpeg')\nimg_cat.show();\n\n\n\n\n\nlearn.predict(img_cat)[0]\n\n\n\n\n'Maine_Coon'\n\n\nThe model correctly predicts that it is a maine coon catâ€”so far so good! Letâ€™s try now with a picture of something it has never seen before, for example an elephant?\n\nimg_el = PILImage.create('elephant.jpg')\nimg_el.show();\n\n\n\n\n\nlearn.predict(img_el)[0]\n\n\n\n\n'British_Shorthair'\n\n\nHere the model tells us that it is a british shorthair cat. Instead, we would like the model to tell us when it doesnâ€™t know something. However, the current model has no way of doing this because it was built that way. Note that when weâ€™ve instantiated our Learner above, we didnâ€™t specify what loss function to use. This is because fastai is smart enough to select the right one for us, which in this case is the cross-entropy loss. Also, to turn the activations into actual predictions, the last layer uses the softmax activation function. By construction, the softmax really wants to pick one class among the availableâ€”after all, the outputs of a softmax layer have to add up to 1 and the largest activation value is magnified even more. In the next section, we will see how by reformulating the problem we can achieve the desired behavior.\n\n\n\n\n\n\nTip\n\n\n\nIf you have not set the loss function yourself and you are unsure of what fastai has chosen for you, you can always inspect the .loss_func attribute of your Learner."
  },
  {
    "objectID": "posts/2022-02-06-unknown-label-problem.html#constructing-a-datablock-and-dataloaders-1",
    "href": "posts/2022-02-06-unknown-label-problem.html#constructing-a-datablock-and-dataloaders-1",
    "title": "The unknown label problem",
    "section": "Constructing a DataBlock and DataLoaders",
    "text": "Constructing a DataBlock and DataLoaders\nIn the multi-class case, we used the CategoryBlock, while here weâ€™ll switch to the MultiCategoryBlock, which returns a one-hot encoded vector. This block expects a list of labels (strings), hence we need to modify a bit the get_y function, which was only returning the pet breed as a string. The next cell defines this function, builds new DataBlock and DataLoaders, and displays a few samples.\n\ndef get_y(fp):\n    \"\"\"Get list of labels from df.\"\"\"\n    lbl = using_attr(RegexLabeller(name_regex), attr=\"name\")(fp)\n    return [lbl]\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(seed=42),\n    get_y=get_y,\n    item_tfms=Resize(460),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75),\n)\n\ndls = dblock.dataloaders(path/\"images\")\ndls.show_batch(nrows=1, ncols=5)"
  },
  {
    "objectID": "posts/2022-02-06-unknown-label-problem.html#model-training-1",
    "href": "posts/2022-02-06-unknown-label-problem.html#model-training-1",
    "title": "The unknown label problem",
    "section": "Model training",
    "text": "Model training\nBefore creating a new model and launching the learning rate finder, weâ€™d like to point out some differences in relation to what we have done above. First, even if we donâ€™t do so explicitly, fastai will again choose the correct loss function for us, which in this case is the binary cross-entropy loss. The activation function of the final layer is no longer the softmax, rather the sigmoid function. The outputs of this layer therefore no longer have to add up to one, but each neuron can return a value between 0 and 1 regardless of the others. This means that we also need to adapt our metric. In the multi-class case, the accuracy was comparing the target label with the class with highest activation (via argmax). But now these activations might even be all 1 in theory, or all 0. We hence need to pick a threshold to decide what has to be set to 0 and what to 1. This is what accuracy_multi does.\n\n# Create CNN model (ResNet18)\nlearn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=0.9))\nlearn.lr_find(start_lr=1e-6)\n\n\n\n\nSuggestedLRs(valley=0.003162277629598975)\n\n\n\n\n\nWe choose again a learning rate value large enough to have a reasonably fast training and launch it.\n\n# Fine-tune model\nlearn.fine_tune(4, base_lr=1e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.319909\n      0.029859\n      0.981696\n      01:30\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.034611\n      0.028626\n      0.986578\n      01:33\n    \n    \n      1\n      0.030179\n      0.023405\n      0.988754\n      01:33\n    \n    \n      2\n      0.018344\n      0.015487\n      0.992576\n      01:32\n    \n    \n      3\n      0.010457\n      0.013025\n      0.993472\n      01:32\n    \n  \n\n\n\nThe results seem promising, so we move on and display a few predictions on the validation set.\n\n# Display couple of results\nlearn.show_results(max_n=5, nrows=1)\n\n\n\n\n\n\n\nThe crucial moment is approaching: we want to see how the model behaves when we show it new pictures and, in particular, pictures depicting things it doesnâ€™t know. We hence repeat the previous experiment by passing it the image of a cat.\n\nlearn.predict(img_cat)[0]\n\n\n\n\n(#1) ['Maine_Coon']\n\n\nAll good for now, it correctly predicts that itâ€™s a maine coon. But what about the image of the elephant?\n\nlearn.predict(img_el)[0]\n\n\n\n\n(#0) []\n\n\nGreat! Our model doesnâ€™t recognise the content of this picture and hence doesnâ€™t return any label! ðŸŽ‰"
  },
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary.html",
    "href": "posts/2021-07-27-gensim-shared-vocabulary.html",
    "title": "Gensim: share vocabulary across models",
    "section": "",
    "text": "Deep learning models based on the transformer architecture have taken the NLP world by storm in the last few years, achieving state-of-the-art results in several areas. An obvious example of this success is provided by the tremendous growth of the Hugging Face ecosystem, which provides access to a plethora of pre-trained models in a very user-friendly way.\nHowever, we believe that models based on (static) word embeddings still have their place in the â€œtransformer eraâ€. Some reasons why this might be the case are the following:\n\nTransformer-based models are usually much bigger (i.e.Â more parameters) than â€œstandardâ€ models.\nTransformer models are not renowned for their (inference) speedâ€”this is related to the previous point.\nModels based on word embeddings still provide a solid baseline.\n\nGensim is a great library when it comes to word embeddings, and some other NLP tasks, especially if you want to train them on your own. There might be cases where you would like to train two NLP models and have them â€œspeak the same languageâ€, i.e.Â share the same vocabulary. For the sake of concreteness, letâ€™s say these two models are LSI and word2vec. The â€œstandardâ€ way of doing this, however, requires the preparation of two vocabularies, one for each model. In this post, weâ€™ll show how to avoid this by transferring the vocabuly of the LSI model to the word2vec model."
  },
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary.html#lsi-model",
    "href": "posts/2021-07-27-gensim-shared-vocabulary.html#lsi-model",
    "title": "Gensim: share vocabulary across models",
    "section": "LSI model",
    "text": "LSI model\nThe first step in building an LSI model is to create a dictionary, which maps words to integer ids. This is easily achieved through the Dictionary class, to which we have to pass tokenised documents:\n\ntokenized_data = [tokenizer(doc) for doc in data]\ndct = Dictionary(tokenized_data)\n\nWith the help of the dictionary we can then build our corpus using the .doc2bow() method. This returns documents in a bag-of-words (BoW) representation. We could proceed with it, but a TF-IDF representation is preferable, for which we can use the TfidfModel class.\n\ncorpus = [dct.doc2bow(line) for line in tokenized_data]\ntfidf_model = TfidfModel(corpus, id2word=dct)\ntfidf_matrix = tfidf_model[corpus]\n\nWe have everything we need to build our LSI model, which is conveniently done by the LsiModel class. Without further motivating this arbitrary choice, we set the number of latent dimensions to 200.\n\n%%time\n\ndim_lsi = 200  # Topic number (latent dimension)\nlsi_model = LsiModel(corpus=tfidf_matrix, id2word=dct, num_topics=dim_lsi)\n\nCPU times: user 12.6 s, sys: 625 ms, total: 13.2 s\nWall time: 10.5 s\n\n\nWe now have an LSI model ready to be used! Letâ€™s move on to word2vec."
  },
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary.html#word2vec-model",
    "href": "posts/2021-07-27-gensim-shared-vocabulary.html#word2vec-model",
    "title": "Gensim: share vocabulary across models",
    "section": "word2vec model",
    "text": "word2vec model\nThe quickest way to train a word2vec model is through the Word2Vec class.\n\n\nCode\ndim_w2v = dim_lsi  # Diminsionality of word vectors\nalpha = 0.025  # Initial learning rate\nalpha_min = 0.0001  # Drop learning rate to this value\nwnd = 5        # Window size (max. distance to predicted word)\nmincount = 2   # Word frequency lower bound\nsample = 1e-5  # Threshold for downsampling\nsg = 1         # Index 1 => Skip-Gram algo.\nngt = 10       # No. noisy words for negative sampling\nepochs = 5     # No. epochs for training\ncpus = multiprocessing.cpu_count()  # Tot. no. of CPUs\nthreads = cpus -1  # Use this number of threads for training\n\n\n\n%%time\n\nw2v_model = Word2Vec(\n    sentences=tokenized_data, vector_size=dim_w2v, alpha=alpha, min_alpha=alpha_min, window=wnd, \n    min_count=mincount, sample=sample, sg=sg, negative=ngt, epochs=epochs, workers=threads\n)\n\nCPU times: user 1min 31s, sys: 1.19 s, total: 1min 32s\nWall time: 53.7 s\n\n\nLetâ€™s double-check the number of words present in each of our two models:\n\n\nCode\nprint('Size of LSI vocab.:', len(dct.keys()))\nprint('Size of w2v vocab.:', len(w2v_model.wv.key_to_index.keys()))\n\n\nSize of LSI vocab.: 42439\nSize of w2v vocab.: 42439\n\n\nWeâ€™ve hence managed to build an LSI and a word2vec model whose vocabularies contain the exact same wordsâ€”great! However, this came at an unnecessarily high price and weâ€™ll shortly see why. What happens behind the scenes when we create a new instance of the Word2Vec class is the following. First a quick sanity check of the corpus is performed, then the vocabulary is built using the .build_vocab() method, and lastly the method .train() is executed, which trains the model. In the second step, a new dictionary is built from scratch, despite having already done so for the LSI model. When working with small datasets this procedure might be acceptable, but when the corpus is very large optimising this step can save a lot of time!"
  },
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary.html#lsi-model-1",
    "href": "posts/2021-07-27-gensim-shared-vocabulary.html#lsi-model-1",
    "title": "Gensim: share vocabulary across models",
    "section": "LSI model",
    "text": "LSI model\nAs weâ€™ve seen before, the first step is to create a dictionary. Before we passed a list to Dictionary, now we pass it a generator:\n\ncurpus_path = Path('20news.txt')\ndct = Dictionary((tokenizer(line) for line in open(curpus_path)))\n\nStep two consists in creating a corpus and switching to a TF-IDF representation. Here is where things change a bit. We need to define an iterable that yields documents in BoW representation, which is done by the Corpus class here below.\n\nclass Corpus:\n    '''Iterable that yields BoW representations of documents.'''\n    \n    def __init__(self, curpus_path, dct_object):\n        self.curpus_path = curpus_path\n        self.dct_object = dct_object\n        \n    def __iter__(self):\n        for line in sopen(self.curpus_path):\n            yield self.dct_object.doc2bow(tokenizer(line))\n\nWe then use it to create our streamed corpus, which can be passed to TfidfModel. Weâ€™ll skip the explicit creation of the TF-IDF matrix because it can be very large.\n\ncorpus = Corpus(curpus_path, dct)\ntfidf_model = TfidfModel(corpus, id2word=dct)\n\nWe are now ready to build our LSI model:\n\n%%time\n\nlsi_model = LsiModel(corpus=tfidf_model[corpus], id2word=dct, num_topics=dim_lsi)\n\nCPU times: user 3min 58s, sys: 11 s, total: 4min 9s\nWall time: 3min 17s\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe need to use iterables and not generators even though they both produce an iterator. This is because after we have exhausted a generator once there is no more data available. In contrast, iterables create a new iterator every time they are looped over. This is exactly what we need when creating a model: we need to be able to iterate over a dataset more than once."
  },
  {
    "objectID": "posts/2021-07-27-gensim-shared-vocabulary.html#word2vec-model-1",
    "href": "posts/2021-07-27-gensim-shared-vocabulary.html#word2vec-model-1",
    "title": "Gensim: share vocabulary across models",
    "section": "word2vec model",
    "text": "word2vec model\nSimilar to what we did with the LSI model, we need to define an iterable that yields tokenized documents. This is provided by the CorpusW2V class below.\n\nclass CorpusW2V:\n    '''Iterable that yields sentences (lists of str).'''\n\n    def __init__(self, curpus_path):\n        self.curpus_path = curpus_path\n\n    def __iter__(self):\n        for line in sopen(self.curpus_path):\n            yield tokenizer(line)\n\ncorpus_w2v = CorpusW2V(curpus_path)\n\nThe rest follows exactly as above, with the only difference that now the .train() method receives an instance of the CorpusW2V class instead of a list (see tokenized_data above).\n\n%%time\n\nw2v_model = Word2Vec(\n    vector_size=dim_w2v, alpha=alpha, min_alpha=alpha_min, window=wnd, \n    min_count=mincount, sample=sample, sg=sg, negative=ngt, workers=threads\n)\n# Borrow LSI vocab.\nword_freq = {dct[k]: v for k,v in dct.cfs.items()}\nw2v_model.build_vocab_from_freq(word_freq)\n# Train model\nnum_samples = dct.num_docs\nw2v_model.train(corpus_w2v, total_examples=num_samples, epochs=epochs)\n\nCPU times: user 5min 52s, sys: 4.78 s, total: 5min 56s\nWall time: 6min 31s\n\n\n(2608743, 6277620)\n\n\nWe conclude by noting that this approach based on data streaming is certainly slower than when we load everything into memory. However, it allows us to process arbitrarily large datasets. One canâ€™t have it all, as they say."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ðŸ“® Posts",
    "section": "",
    "text": "The unknown label problem\n\n\n\n\n\n\n\nfastai\n\n\ncomputer vision\n\n\nmulti-class classification\n\n\nmulti-label classification\n\n\n\n\nWhy turning a multi-class classification task into a multi-label one might be a good idea.\n\n\n\n\n\n\nFeb 6, 2022\n\n\nLuca Papariello\n\n\n\n\n\n\n\n\nGensim: share vocabulary across models\n\n\n\n\n\n\n\nnlp\n\n\ngensim\n\n\n\n\nA brief illustration of how to share a common vocabulary among different Gensim models.\n\n\n\n\n\n\nJul 27, 2021\n\n\nLuca Papariello\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "After repeatedly finding help in a very welcoming community, I thought it was time to give back some of that warmth through this blog. It can be seen as a collection of personal notes and/or tricks discovered while working on my projects."
  }
]